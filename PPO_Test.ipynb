{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "import os\n",
    "os.environ[\"NGSIM_DIR\"] = \"/home/surender/Downloads/NGSIM\"\n",
    "os.environ[\"OPENDD_DIR\"] = \"/home/surender/Downloads/openDD\"\n",
    "os.environ[\"CARLA_PATH\"] = \"/home/surender/Downloads/carlaOld\"\n",
    "import sys\n",
    "#sys.path.append('/home/surender/Downloads/CARLA_0.9.9.4/PythonAPI/carla/dist')\n",
    "import carla\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "from carla_real_traffic_scenarios.carla_maps import CarlaMaps\n",
    "from carla_real_traffic_scenarios.ngsim import NGSimDatasets, DatasetMode\n",
    "from carla_real_traffic_scenarios.ngsim.scenario import NGSimLaneChangeScenario\n",
    "from carla_real_traffic_scenarios.opendd.scenario import OpenDDScenario\n",
    "from carla_real_traffic_scenarios.reward import RewardType\n",
    "from carla_real_traffic_scenarios.scenario import Scenario\n",
    "\n",
    "from carla_birdeye_view import BirdViewProducer, BirdViewCropType, PixelDimensions\n",
    "from PIL import Image\n",
    "#from IPython.display import clear_output, Image, display, HTML\n",
    "import cv2\n",
    "\n",
    "%matplotlib tk\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(torch.cuda.is_available()): \n",
    "    device = torch.device('cuda:0') \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print(\"Device set to : cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "\n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class Flatten(nn.Module):\n",
    "    \"\"\"Helper to flatten a tensor.\"\"\"\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "'''\n",
    "class Normal(nn.Module):\n",
    "    \"\"\"A module that builds a Diagonal Gaussian distribution from means.\n",
    "    Standard deviations are learned parameters in this module.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_outputs):\n",
    "        super().__init__()\n",
    "        # initial variance is e^0 = 1\n",
    "        self.stds = nn.Parameter(torch.zeros(num_outputs))\n",
    "\n",
    "    def forward(self, x):\n",
    "        dist = torch.distributions.Normal(loc=x, scale=self.stds.exp())\n",
    "\n",
    "        # By default we get the probability of sampling each dimension of the\n",
    "        # distribution. The full probability is the product of these, or\n",
    "        # the sum since we're working with log probabilities.\n",
    "        # So overwrite the log_prob function to handle this for us\n",
    "        dist.old_log_prob = dist.log_prob\n",
    "        dist.log_prob = lambda x: dist.old_log_prob(x).sum(-1)\n",
    "\n",
    "        return dist\n",
    "    \n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self,num_channels=3):\n",
    "        super(AutoEncoder,self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 16, kernel_size=2, stride=2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 16, kernel_size=1, stride=1),\n",
    "            \n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.l1 = nn.ConvTranspose2d(16, 16, kernel_size = 8, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.ConvTranspose2d(16, 32, kernel_size = 8, stride=2)\n",
    "        self.l3 = nn.ConvTranspose2d(32, 3, kernel_size = 8, stride=1)\n",
    "        self.l4 = nn.ConvTranspose2d(3,3,kernel_size = 20,stride = 5)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        #x = self.flatten(x)\n",
    "        #print(x.shape)\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        #print(x.shape)\n",
    "        x = self.l2(x)\n",
    "        x = self.relu(x)\n",
    "        #print(x.shape)\n",
    "        x = self.l3(x)\n",
    "        x = self.relu(x)\n",
    "        #print(x.shape)\n",
    "        x = self.l4(x)\n",
    "        \n",
    "        x = self.sig(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "        \n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init,hidden_size = 512,num_channels = 5):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_dim = action_dim\n",
    "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, 32, kernel_size=8, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(4,2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2,1),\n",
    "            #nn.Conv2d(128, 256, kernel_size=4, stride=1),\n",
    "            #nn.BatchNorm2d(256),\n",
    "            #nn.MaxPool2d(2,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(124416, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size,action_dim),\n",
    "            nn.Tanh()\n",
    "            \n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, 32, kernel_size=8, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(4,2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2,1),\n",
    "            #nn.Conv2d(128, 256, kernel_size=4, stride=1),\n",
    "            #nn.BatchNorm2d(256),\n",
    "            #nn.MaxPool2d(2,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(124416, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size,1),\n",
    "            nn.Tanh()\n",
    "            \n",
    "            \n",
    "        )\n",
    "        \n",
    "        \n",
    "        #self.l1 = nn.Conv2d(num_channels, 32, kernel_size=4, stride=2)\n",
    "        #self.l2 = nn.Conv2d(32, 16, kernel_size=4, stride=2)\n",
    "        #self.l3 = nn.Conv2d(16, 8, kernel_size=3, stride=1)\n",
    "        #self.p = nn.MaxPool2d(2, 2)\n",
    "        ''''\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 16, kernel_size=4, stride=2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, stride=1),\n",
    "            \n",
    "            #nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.l1 = nn.ConvTranspose2d(16, 16, kernel_size = 8, stride=4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.ConvTranspose2d(16, 32, kernel_size = 8, stride=1)\n",
    "        self.l3 = nn.ConvTranspose2d(32, 3, kernel_size = 8, stride=2)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "        self.layer1 = nn.Conv2d(num_channels, 32, kernel_size=8, stride=4)\n",
    "        self.relu = nn.ReLU();\n",
    "        self.layer2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.layer3 = nn.Conv2d(64, 32, kernel_size=3, stride=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer4 = nn.Linear(9120,512)\n",
    "        self.actol = nn.Linear(512, action_dim)\n",
    "        self.acto = nn.Tanh()\n",
    "        self.crit = nn.Linear(512, 1)\n",
    "        '''\n",
    "        #self.actor = nn.Sequential(self.main,nn.Linear(hidden_size, action_dim))\n",
    "        #self.critic = nn.Sequential(self.main_two,nn.Linear(hidden_size, 1))\n",
    "        '''\n",
    "        # actor\n",
    "        if has_continuous_action_space :\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Tanh()\n",
    "                        )\n",
    "        else:\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Linear(state_dim, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                            nn.Softmax(dim=-1)\n",
    "                        )\n",
    "\n",
    "        \n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "                        nn.Linear(state_dim, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 1)\n",
    "                    )\n",
    "        '''\n",
    "        \n",
    "    def set_action_std(self, new_action_std):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    def AutoEncoder(self,x):\n",
    "        '''\n",
    "        x = self.encoder(x)\n",
    "        print(x.shape)\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        print(x.shape)\n",
    "        x = self.l2(x)\n",
    "        x = self.relu(x)\n",
    "        print(x.shape)\n",
    "        x = self.l3(x)\n",
    "        x = self.sig(x)\n",
    "        print(x.shape)\n",
    "        \n",
    "        print(x.shape)\n",
    "        x = self.l1(x)\n",
    "        print(x.shape)\n",
    "        x = self.p(x)\n",
    "        print(x.shape)\n",
    "        x = self.l2(x)\n",
    "        print(x.shape)\n",
    "        x = self.p(x)\n",
    "        print(x.shape)\n",
    "        x = self.l3(x)\n",
    "        print(x.shape)\n",
    "        x = self.p(x)\n",
    "        print(x.shape)\n",
    "        '''\n",
    "        #x = self.encoder(x)\n",
    "        #return x\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "        '''\n",
    "        print(x.shape)\n",
    "        x = self.layer1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.relu(x)\n",
    "        print(x.shape)\n",
    "        x = self.layer2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.relu(x)\n",
    "        print(x.shape)\n",
    "        x = self.layer3(x)\n",
    "        #print(x.shape)\n",
    "        x = self.relu(x)\n",
    "        print(x.shape)\n",
    "        x = self.flatten(x)\n",
    "        print(x.shape)\n",
    "        x = self.layer4(x)\n",
    "        print(x.shape)\n",
    "        a = self.actol(x)\n",
    "        a = self.acto(a)\n",
    "        v = self.crit(x)\n",
    "        return a,v\n",
    "        '''\n",
    "        \n",
    "    def backward(self, x):\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            #x = self.main(state)\n",
    "            action_mean = self.actor(state)\n",
    "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        \n",
    "        return action.detach(), action_logprob.detach()\n",
    "    \n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            if len(state.shape) == 3:\n",
    "                state = state.reshape((1,5,186,150))\n",
    "            #x = self.main(state)\n",
    "            #print(state.shape)\n",
    "            action_mean = self.actor(state)\n",
    "            #print(action_mean.shape)\n",
    "            action_var = self.action_var.expand_as(action_mean)\n",
    "            #print(action_var,action_var.shape)\n",
    "            cov_mat = torch.diag_embed(action_var).to(device)\n",
    "            #print(cov_mat)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "            #print(dist)\n",
    "            # for single action continuous environments\n",
    "            if self.action_dim == 1:\n",
    "                action = action.reshape(-1, self.action_dim)\n",
    "\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(state)\n",
    "        #print(state_values,action_mean)\n",
    "        return action_logprobs, state_values, dist_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_std = action_std_init\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "                        \n",
    "                        {'params': self.policy.critic.parameters(),'lr': lr_critic}\n",
    "                        \n",
    "                    ])\n",
    "        self.optimizer2 = torch.optim.Adam([\n",
    "            {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "        ])\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        \n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = new_action_std\n",
    "            self.policy.set_action_std(new_action_std)\n",
    "            self.policy_old.set_action_std(new_action_std)\n",
    "        \n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
    "        #print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = self.action_std - action_std_decay_rate\n",
    "            self.action_std = round(self.action_std, 4)\n",
    "            if (self.action_std <= min_action_std):\n",
    "                self.action_std = min_action_std\n",
    "                #print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
    "            else:\n",
    "                pass\n",
    "                #print(\"setting actor output action_std to : \", self.action_std)\n",
    "            self.set_action_std(self.action_std)\n",
    "\n",
    "        else:\n",
    "            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n",
    "\n",
    "        #print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob = self.policy_old.act(state)\n",
    "\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "\n",
    "            return action.detach().cpu().numpy().flatten()\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob = self.policy_old.act(state)\n",
    "            \n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "\n",
    "            return action.item()\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        #print(self.buffer.rewards)\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        #print(\"I\",rewards)\n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        #print(rewards,rewards.mean(),rewards.std(unbiased = False))\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std(unbiased = False) + 1e-7)\n",
    "        #print(rewards)\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
    "\n",
    "        \n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "            #print(state_values)\n",
    "            # match state_values tensor dimensions with rewards tensor\n",
    "            state_values = torch.squeeze(state_values)\n",
    "            \n",
    "            #state_values = torch.\n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "            #print(ratios)\n",
    "            # Finding Spurrogate Loss\n",
    "            advantages = rewards - state_values.detach()   \n",
    "            #print(rewards)\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            #print(state_values)\n",
    "            #print(state_values.shape)\n",
    "            # final loss of clipped objective PPO\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "            #print(surr1,surr2,state_values,rewards,dist_entropy,loss)\n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            #self.optimizer2.step()\n",
    "            \n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # clear buffer\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    \n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "   \n",
    "\n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_timestep = 1000     # update policy every n timesteps\n",
    "K_epochs = 40               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "random_seed = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ngsim_scenario(client: carla.Client) -> Scenario:\n",
    "    data_dir = os.environ.get(\"NGSIM_DIR\")\n",
    "    #data_dir = os.listdir('/home/surender/Downloads/NGSIM')\n",
    "    assert data_dir, \"Path to the directory with NGSIM dataset is required\"\n",
    "    ngsim_map = NGSimDatasets.list()\n",
    "    ngsim_dataset = ngsim_map[1]\n",
    "    client.load_world(ngsim_dataset.carla_map.level_path)\n",
    "    return NGSimLaneChangeScenario(\n",
    "        ngsim_dataset,\n",
    "        dataset_mode=DatasetMode.TRAIN,\n",
    "        data_dir=data_dir,\n",
    "        reward_type=RewardType.DENSE,\n",
    "        client=client,\n",
    "    )\n",
    "\n",
    "'''\n",
    "def prepare_opendd_scenario(client: carla.Client) -> Scenario:\n",
    "    data_dir = os.environ.get(\"OPENDD_DIR\")\n",
    "    assert data_dir, \"Path to the directory with openDD dataset is required\"\n",
    "    maps = [\"rdb1\", \"rdb2\", \"rdb3\", \"rdb4\", \"rdb5\", \"rdb6\", \"rdb7\"]\n",
    "    map_name = random.choice(maps)\n",
    "    carla_map = getattr(CarlaMaps, map_name.upper())\n",
    "    client.load_world(carla_map.level_path)\n",
    "    return OpenDDScenario(\n",
    "        client,\n",
    "        dataset_dir=data_dir,\n",
    "        dataset_mode=DatasetMode.TRAIN,\n",
    "        reward_type=RewardType.DENSE,\n",
    "        place_name=map_name,\n",
    "    )\n",
    "\n",
    "'''\n",
    "def prepare_ego_vehicle(world: carla.World) -> carla.Actor:\n",
    "    car_blueprint = world.get_blueprint_library().find(\"vehicle.audi.a2\")\n",
    "\n",
    "    # This will allow external scripts like manual_control.py or no_rendering_mode.py\n",
    "    # from the official CARLA examples to take control over the ego agent\n",
    "    car_blueprint.set_attribute(\"role_name\", \"hero\")\n",
    "\n",
    "    # spawn points doesnt matter - scenario sets up position in reset\n",
    "    ego_vehicle = world.spawn_actor(\n",
    "        car_blueprint, carla.Transform(carla.Location(0, 0, 500), carla.Rotation())\n",
    "    )\n",
    "\n",
    "    assert ego_vehicle is not None, \"Ego vehicle could not be spawned\"\n",
    "\n",
    "    # Setup any car sensors you like, collect observations and then use them as input to your model\n",
    "    return ego_vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmd_carla():\n",
    "    os.system(\"DISPLAY= /home/surender/Downloads/carlaOld/CarlaUE4.sh -benchmark -fps=10 -quality-level=Low -opengl -Resx=300 -Resy=300 -NoVSync \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmd_carla():\n",
    "    os.system(\"DISPLAY= /home/surender/Downloads/carlaOld/CarlaUE4.sh -benchmark -fps=5 -quality-level=Low -opengl -Resx=4 -Resy=4 -NoVSync\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = threading.Thread(target = cmd_carla)\n",
    "p.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"localhost\"\n",
    "port = 2000\n",
    "client = carla.Client(host,port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.squeeze(torch.tensor([[1.4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = prepare_ngsim_scenario(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = client.get_world()\n",
    "spectator = world.get_spectator()\n",
    "ego_vehicle = prepare_ego_vehicle(world)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = world.get_settings()\n",
    "settings.no_rendering_mode = True\n",
    "world.apply_settings(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_frame = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_img(img):\n",
    "    global input_data,current_frame\n",
    "    c_img = img\n",
    "    #print(img.frame)\n",
    "    array = np.frombuffer(img.raw_data, dtype=np.dtype(\"uint8\"))\n",
    "    #print(array.shape)\n",
    "    array = np.reshape(array, (img.height, img.width, 4)) # RGBA format\n",
    "    array = array[:, :, :3] #  Take only RGB\n",
    "    #print(array.shape)\n",
    "    #plt.imshow(array)\n",
    "    \n",
    "    img = Image.fromarray(array)\n",
    "    \n",
    "    #print(img)\n",
    "    img = img.resize((320,320), Image.ANTIALIAS)\n",
    "    #print(img)\n",
    "    input_data = np.array(img)\n",
    "    current_frame = c_img.frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_bp = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "cam_bp.set_attribute(\"image_size_x\",str(320))\n",
    "cam_bp.set_attribute(\"image_size_y\",str(320))\n",
    "cam_bp.set_attribute(\"fov\",str(100))\n",
    "cam_location = carla.Location(2,0,1)\n",
    "cam_rotation = carla.Rotation(0,0,0)\n",
    "cam_transform = carla.Transform(cam_location,cam_rotation)\n",
    "ego_front_cam = world.spawn_actor(cam_bp,cam_transform,attach_to=ego_vehicle, attachment_type=carla.AttachmentType.Rigid)\n",
    "#self.rgb_front_listener = ego_cam\n",
    "ego_front_cam.listen(lambda image: check_img(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario.reset(ego_vehicle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario.step(ego_vehicle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = world.tick()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario._target_lane_waypoint.transform.location.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.tick()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PPO(1,2,lr_actor,lr_critic,gamma,K_epochs,eps_clip,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.load(\"Model_CHK4.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = AutoEncoder(3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = ae(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.reshape((3,320,320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.reshape((320,320,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birdview_producer = BirdViewProducer(\n",
    "    client,  # carla.Client\n",
    "    target_size=PixelDimensions(width=150, height=186),\n",
    "    pixels_per_meter=4,\n",
    "    crop_type=BirdViewCropType.FRONT_AREA_ONLY\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birdview = birdview_producer.produce(\n",
    "            agent_vehicle=ego_vehicle  # carla.Actor (spawned vehicle)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_vehicle.get_location().y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario._target_lane_waypoint.transform.location.y - ego_vehicle.get_location().y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario._target_lane_waypoint.transform.location.x - ego_vehicle.get_location().x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_vehi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = ego_vehicle.get_velocity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ego_vehicle.apply_control(carla.VehicleControl(throttle=1.0, steer=0.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario.step(ego_vehicle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.tick()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birdview = birdview_producer.produce(\n",
    "            agent_vehicle=ego_vehicle  # carla.Actor (spawned vehicle)\n",
    "            )\n",
    "rgb = BirdViewProducer.as_rgb(birdview)\n",
    "cv2.imshow('Frame',rgb)\n",
    "\n",
    "if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(birdview[0]) #Full Road Greyed out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(birdview[1])  #Lanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(birdview[2]) #Centerlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(birdview[3])#Other vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(birdview[4])# Ego agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data = birdview[:5,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data = in_data.reshape((1,5,186,150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(in_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.select_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.policy.actor(state.to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.policy.forward(torch.FloatTensor(in_data).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.policy.actor.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    state = torch.FloatTensor(in_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = p.policy.AutoEncoder(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = AutoEncoder(3).to(device)\n",
    "optimizer = torch.optim.Adam([\n",
    "                        {'params': ae.parameters(), 'lr': 0.003}\n",
    "])\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.cat((state,state), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del inp_tensor,outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A =  state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "min_batch_size = 32\n",
    "inp_tensor = state\n",
    "ep_list = []\n",
    "loss_list = []\n",
    "for epoch in range(epochs):\n",
    "    step = 0\n",
    "    scenario.reset(ego_vehicle)\n",
    "    c = world.tick()\n",
    "    done = False\n",
    "    total_r = 0\n",
    "    val = 0\n",
    "    \n",
    "    while not done:\n",
    "        in_data = input_data.reshape((1,3,320,320))\n",
    "        with torch.no_grad():\n",
    "            st = torch.FloatTensor(in_data)\n",
    "        ego_vehicle.apply_control(carla.VehicleControl(throttle=0.5))\n",
    "        try:\n",
    "            cmd, reward, done, _ = scenario.step(ego_vehicle)\n",
    "        except:\n",
    "            break\n",
    "        c = world.tick()\n",
    "        inp_tensor = torch.cat((inp_tensor,st),0)\n",
    "        del st\n",
    "        if inp_tensor.shape[0] >= min_batch_size:\n",
    "            break\n",
    "        step += 1\n",
    "    if inp_tensor.shape[0] >= min_batch_size:\n",
    "            optimizer.zero_grad()\n",
    "            A = inp_tensor/255.\n",
    "            #A -= A.min(1, keepdim=True)[0]\n",
    "            #A /= A.max(1, keepdim=True)[0]\n",
    "            inp_tensor = A\n",
    "            outputs = ae(inp_tensor.to(device))\n",
    "            loss = criterion(outputs, inp_tensor.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"EPOCH:\",epoch,\"LOSS:\",loss.item())\n",
    "            ep_list.append(epoch)\n",
    "            loss_list.append(loss.item())\n",
    "            del inp_tensor,outputs,A\n",
    "            inp_tensor = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BCE_loss = [ep_list,loss_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_loss_1 = [ep_list,loss_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"MSE1.pkl\",'wb')\n",
    "pickle.dump(MSE_loss_1,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.ylim(0,1)\n",
    "plt.plot(ep_list,loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data = state.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_img = in_data.reshape((3,320,320)).reshape((320,320,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(i_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out =ae(state.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oupp = out.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_img = oupp.reshape((3,320,320)).reshape((320,320,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(o_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward_list = []\n",
    "epoch_list = []\n",
    "step_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "freq = 64\n",
    "freq_n = 3\n",
    "for epoch in range(epochs):\n",
    "    step = 0\n",
    "    scenario.reset(ego_vehicle)\n",
    "    c = world.tick()\n",
    "    done = False\n",
    "    total_r = 0\n",
    "    val = 0\n",
    "    \n",
    "    \n",
    "    t_clip_n = 0.0\n",
    "    t_clip_p = 1.0\n",
    "    \n",
    "    s_clip_n = -1.0\n",
    "    s_clip_p = 1.0\n",
    "    \n",
    "    \n",
    "    while not done:\n",
    "        '''\n",
    "        while True:\n",
    "            #print(current_frame,c)\n",
    "            if current_frame >= c:\n",
    "                #print(current_frame,c)\n",
    "                break\n",
    "        '''\n",
    "        birdview = birdview_producer.produce(\n",
    "            agent_vehicle=ego_vehicle  # carla.Actor (spawned vehicle)\n",
    "            )\n",
    "        in_data = birdview[:5,:,:]\n",
    "        in_data = in_data.reshape((1,5,186,150))\n",
    "        #in_data = input_data.reshape((1,3,320,320))\n",
    "        action = p.select_action(in_data)\n",
    "        #print(action)\n",
    "        '''\n",
    "        if (val == 0  or val ==1):\n",
    "            s_clip_n = -0.15\n",
    "            s_clip_p = 0.15\n",
    "            t_clip_n = 0.4\n",
    "            t_clip_p = 1.0\n",
    "        \n",
    "        if (val == 2 or val == 5):\n",
    "            s_clip_n = 0.25\n",
    "            s_clip_p = 0.8\n",
    "            t_clip_n = 0.0\n",
    "            t_clip_p = 0.4\n",
    "        \n",
    "        if (val == 3 or val == 4):\n",
    "            s_clip_n = -0.8\n",
    "            s_clip_p = -0.25\n",
    "            t_clip_n = 0.0\n",
    "            t_clip_p = 0.4\n",
    "        '''\n",
    "            \n",
    "        \n",
    "        t_clip_n = 0.0\n",
    "        t_clip_p = 1.0\n",
    "\n",
    "        s_clip_n = -1.0\n",
    "        s_clip_p = 1.0    \n",
    "        \n",
    "        brake = 0\n",
    "        throttle = 0\n",
    "        if action[0] <0:\n",
    "            brake = action[0]\n",
    "            throttle = 0\n",
    "        else:\n",
    "            throttle = action[0]\n",
    "            brake = 0\n",
    "        \n",
    "        if (val == 0  or val ==1):\n",
    "            s_clip_n = -0.15\n",
    "            s_clip_p = 0.15\n",
    "            t_clip_n = 0.4\n",
    "            t_clip_p = 1.0\n",
    "        \n",
    "        if (val == 2 or val == 5):\n",
    "            s_clip_n = 0.25\n",
    "            s_clip_p = 0.8\n",
    "            t_clip_n = 0.0\n",
    "            t_clip_p = 0.4\n",
    "        \n",
    "        if (val == 3 or val == 4):\n",
    "            s_clip_n = -0.8\n",
    "            s_clip_p = -0.25\n",
    "            t_clip_n = 0.0\n",
    "            t_clip_p = 0.4\n",
    "        \n",
    "        #if epoch < 20:\n",
    "        ego_vehicle.apply_control(carla.VehicleControl(throttle=np.clip(throttle, t_clip_n, t_clip_p), steer=np.clip(action[1], s_clip_n, s_clip_p),brake=np.clip(brake, 0.0, 1.0)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        cmd, reward, done, _ = scenario.step(ego_vehicle)\n",
    "        val = cmd.value\n",
    "        #print(done)\n",
    "        #if done:\n",
    "        #    print(_)\n",
    "        #print(_)\n",
    "        \n",
    "        \n",
    "        v = ego_vehicle.get_velocity()\n",
    "        kmh = int(3.6 * math.sqrt(v.x**2 + v.y**2 + v.z**2))\n",
    "        \n",
    "        if kmh < 60 & kmh > 0.2:\n",
    "            #done = False\n",
    "            reward += 1 #-1\n",
    "            # Reward lighter steering when moving\n",
    "            if np.abs(action[1]) < 0.3:\n",
    "                reward += 1\n",
    "            elif np.abs(action[1]) > 0.5 and np.abs(action[1]) < 0.9:\n",
    "                reward -= 0.1\n",
    "            elif np.abs(action[1]) >= 0.9:\n",
    "                reward -= 0.2\n",
    "        elif kmh < 0.2:\n",
    "            reward -= 0.1\n",
    "        else:\n",
    "            #print(\"Maybe never\")\n",
    "            reward += 0.01\n",
    "            if np.abs(action[1]) < 0.3:\n",
    "                reward += 0.12\n",
    "            # Reduce score for heavy steering\n",
    "            if np.abs(action[1]) > 0.5 and np.abs(action[1]) < 0.9:\n",
    "                reward -= 0.17\n",
    "            elif np.abs(action[1]) >= 0.9:\n",
    "                reward -= 0.21\n",
    "        \n",
    "        '''\n",
    "        rgb = BirdViewProducer.as_rgb(birdview)\n",
    "        cv2.imshow('Frame',rgb)\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "        '''\n",
    "        p.buffer.rewards.append(reward)\n",
    "        p.buffer.is_terminals.append(done)\n",
    "        \n",
    "        total_r += reward\n",
    "        step += 1\n",
    "        \n",
    "        if step % freq ==0 :\n",
    "            print(step)\n",
    "            p.update()\n",
    "        if step % freq_n == 0:\n",
    "            p.decay_action_std(0.05,0.005)\n",
    "        c = world.tick()\n",
    "    \n",
    "    try:   \n",
    "        p.update()\n",
    "    except Exception as e:\n",
    "        print(\"Error:\",e)\n",
    "        pass\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    print(total_r,epoch,step)\n",
    "    total_reward_list.append(total_r)\n",
    "    epoch_list.append(epoch)\n",
    "    step_list.append(step)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "History = [epoch_list,total_reward_list,step_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(epoch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(epoch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"History5.pkl\",'wb')\n",
    "pickle.dump(History,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.save(\"Model_CHK5.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_list = epoch_list[500:]\n",
    "total_reward_list = total_reward_list[500:]\n",
    "step_list = step_list[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.ylim(-2,1)\n",
    "plt.plot(epoch_list,total_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "320*320*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data = input_data.reshape((1,3,320,320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.select_action(in_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.buffer.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.buffer.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
