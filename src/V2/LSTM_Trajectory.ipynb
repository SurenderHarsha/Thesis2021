{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal,Beta,Dirichlet\n",
    "from torch.distributions import Categorical\n",
    "from torch.distributions.normal import Normal\n",
    "import os\n",
    "os.environ[\"NGSIM_DIR\"] = \"/home/surender/Downloads/NGSIM\"\n",
    "os.environ[\"OPENDD_DIR\"] = \"/home/surender/Downloads/openDD\"\n",
    "os.environ[\"CARLA_PATH\"] = \"/home/surender/Downloads/carlaOld\"\n",
    "import sys\n",
    "#sys.path.append('/home/surender/Downloads/CARLA_0.9.9.4/PythonAPI/carla/dist')\n",
    "import carla\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "from carla_real_traffic_scenarios.carla_maps import CarlaMaps\n",
    "from carla_real_traffic_scenarios.ngsim import NGSimDatasets, DatasetMode\n",
    "from carla_real_traffic_scenarios.ngsim.scenario import NGSimLaneChangeScenario\n",
    "from carla_real_traffic_scenarios.opendd.scenario import OpenDDScenario\n",
    "from carla_real_traffic_scenarios.reward import RewardType\n",
    "from carla_real_traffic_scenarios.scenario import Scenario\n",
    "\n",
    "from carla_birdeye_view import BirdViewProducer, BirdViewCropType, PixelDimensions\n",
    "from PIL import Image\n",
    "#from IPython.display import clear_output, Image, display, HTML\n",
    "import cv2\n",
    "\n",
    "%matplotlib tk\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "import math\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "from controller import VehiclePIDController"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 76\n",
    "torch.manual_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "#np.random.seed(random.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to : GeForce GTX 1060 3GB\n"
     ]
    }
   ],
   "source": [
    "if(torch.cuda.is_available()): \n",
    "    device = torch.device('cuda:0') \n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    \n",
    "    \n",
    "    print(\"Device set to : cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.helper = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "\n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "        del self.helper[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init,hidden_size = 512,num_channels = 5):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_dim = action_dim\n",
    "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
    "        \n",
    "        self.layer1 = nn.LSTM(16, 512,batch_first=True)\n",
    "            \n",
    "        #self.mem = (torch.zeroes)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.ReLU()\n",
    "        \n",
    "        \n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            \n",
    "            \n",
    "            nn.Linear(128, 128),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,12),\n",
    "            nn.Softmax(dim=-1)\n",
    "            \n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(128,1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def act(self, state,val = False):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "\n",
    "            inps1 = state\n",
    "\n",
    "            action_m = self.layer1(inps1)\n",
    "\n",
    "            action_mean = self.actor(action_m)\n",
    "            if val:\n",
    "                return action_mean.detach(),-1\n",
    "            action_log_std = self.action_log_std.expand_as(action_mean)\n",
    "\n",
    "\n",
    "            dist = Normal(action_mean,action_log_std.exp())\n",
    "        else:\n",
    "            #inps1 = state[:,:-3]\n",
    "            #inps2 = state[:,-3:]\n",
    "            action_m,_ = self.layer1(state)\n",
    "            #print(action_m,inps2,action_m.shape,inps2.shape)\n",
    "            #inps = torch.cat((action_m,inps2),1)\n",
    "            action_m = self.layer2(action_m.reshape(1,action_m.shape[-1]))\n",
    "            action_probs = self.actor(action_m)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        #action = action*2 - 1\n",
    "\n",
    "        return action.detach(), action_logprob.detach()\n",
    "    def evaluate(self, state, action):\n",
    "        #print(\"NOTHING\")\n",
    "        \n",
    "        if self.has_continuous_action_space:\n",
    "            '''\n",
    "            if len(state.shape) == 3:\n",
    "                state = state.reshape((1,5,186,150))\n",
    "            '''\n",
    "            #x = self.main(state)\n",
    "            #print(state.shape)\n",
    "            inps1 = state\n",
    "            #inps2 = helper\n",
    "            action_m = self.layer1(inps1)\n",
    "            #inps = torch.cat((action_m,inps2),1)\n",
    "            #x = self.layer2(inps)\n",
    "            \n",
    "            action_mean = self.actor(action_m)\n",
    "            action_log_std = self.action_log_std.expand_as(action_mean)\n",
    "            #action_var = self.var(x) + 1\n",
    "            #action_std = self.std(state)\n",
    "            #print(action_std)\n",
    "            #print(torch.mean(action_std))\n",
    "            #self.set_action_std(torch.mean(action_std))\n",
    "            #var = torch.mean(action_std)\n",
    "            #print(action_mean.shape)\n",
    "            #print(action_mean.shape,action_var.shape)\n",
    "            #action_var = action_var.expand_as(action_mean)\n",
    "            #print(action_var,action_var.shape)\n",
    "            #print(action_mean.shape,action_var.shape)\n",
    "            #cov_mat = torch.diag_embed(action_var)\n",
    "            #print(cov_mat)\n",
    "            dist = Normal(action_mean,action_log_std.exp())\n",
    "            #dist = Beta(action_mean, action_var)\n",
    "            #print(action_mean.shape,cov_var.shape)\n",
    "            #dist = MultivariateNormal(action_mean,cov_mat)\n",
    "            #print(dist)\n",
    "            # for single action continuous environments\n",
    "            #if self.action_dim == 1:\n",
    "            #    action = action.reshape(-1, self.action_dim)\n",
    "\n",
    "        else:\n",
    "            #inps1 = state[:,:-3]\n",
    "            #inps2 = state[:,-3:]\n",
    "            state = state.reshape(state.shape[0],1,state.shape[1])\n",
    "            action_m,_ = self.layer1(state)\n",
    "            #inps = torch.cat((action_m,inps2),1)\n",
    "            #\n",
    "            bsize = state.shape[0]\n",
    "            action_m = self.layer2(action_m.reshape(bsize,action_m.shape[-1]))\n",
    "            action_probs = self.actor(action_m)\n",
    "            dist = Categorical(action_probs)\n",
    "        #action = (action + 1)/2\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        #print(action_logprobs.shape)\n",
    "        dist_entropy = dist.entropy()\n",
    "        inps1 = state\n",
    "        #nps2 = helper\n",
    "        critic_m,_ = self.layer1(inps1)\n",
    "        #inps = torch.cat((critic_m,inps2),1)\n",
    "        #action_mean = self.actor(inps)\n",
    "        critic_m = self.layer2(critic_m.reshape(bsize,critic_m.shape[-1]))\n",
    "        state_values = self.critic(critic_m)\n",
    "        #print(state_values,action_mean)\n",
    "        return action_logprobs, state_values, dist_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_std = action_std_init\n",
    "        self.entropy_c = 0.01\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "                        {'params': self.policy.layer1.parameters(), 'lr': lr_actor},\n",
    "                        {'params': self.policy.layer2.parameters(), 'lr': lr_actor},\n",
    "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "                        #{'params': self.policy.action_log_std, 'lr': 0.01}\n",
    "                        \n",
    "                        \n",
    "                    ])\n",
    "        \n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        \n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = new_action_std\n",
    "            self.policy.set_action_std(new_action_std)\n",
    "            self.policy_old.set_action_std(new_action_std)\n",
    "        \n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
    "        #print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = self.action_std - action_std_decay_rate\n",
    "            self.action_std = round(self.action_std, 4)\n",
    "            if (self.action_std <= min_action_std):\n",
    "                self.action_std = min_action_std\n",
    "                #print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
    "            else:\n",
    "                pass\n",
    "                #print(\"setting actor output action_std to : \", self.action_std)\n",
    "            self.set_action_std(self.action_std)\n",
    "\n",
    "        else:\n",
    "            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n",
    "\n",
    "        #print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    def select_action(self, state,val = False):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            with torch.no_grad():\n",
    "                #helper = torch.FloatTensor(state[1]).to(device)\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                \n",
    "                action, action_logprob,mem = self.policy_old.act(state,val)\n",
    "            if not val:\n",
    "                self.buffer.states.append(state)\n",
    "                #self.buffer.helper.append(helper)\n",
    "                self.buffer.actions.append(action)\n",
    "                self.buffer.logprobs.append(action_logprob)\n",
    "\n",
    "            return action.detach().cpu().numpy().flatten(),mem\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).to(device)\n",
    "                action, action_logprob = self.policy_old.act(state,val)\n",
    "            if not val:\n",
    "                self.buffer.states.append(state)\n",
    "                self.buffer.actions.append(action)\n",
    "                self.buffer.logprobs.append(action_logprob)\n",
    "\n",
    "            return action.item()\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        global tester,te2\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        #print(self.buffer.rewards)\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        #print(\"I\",rewards)\n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        #print(rewards,rewards.mean(),rewards.std(unbiased = False))\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std(unbiased = False) + 1e-7)\n",
    "        #print(rewards)\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
    "        #old_helper = torch.squeeze(torch.stack(self.buffer.helper, dim=0)).detach().to(device)\n",
    "        batch_size = 32\n",
    "        tester = old_states\n",
    "        print(old_states.shape,old_actions.shape,old_logprobs.shape)\n",
    "        #mem = (torch.zeros(1,old_states.shape[0],128).to(device),torch.zeros(1,old_states.shape[0],128).to(device))\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "            #for index in BatchSampler(SubsetRandomSampler(range(len(self.buffer.states))), batch_size, False):\n",
    "            # Evaluating old actions and values\n",
    "                #print(index,old_actions[index],old_actions)\n",
    "            #print(old_states[index].shape)\n",
    "            #te2 = index\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "            #temp_size = logprobs.shape[0]\n",
    "            #print(state_values.shape)\n",
    "            #print(logprobs.shape,logprobs)\n",
    "            # match state_values tensor dimensions with rewards tensor\n",
    "            state_values = torch.squeeze(state_values)\n",
    "            #print(state_values.shape)\n",
    "            #state_values = torch.\n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "            #print(ratios)\n",
    "            # Finding Spurrogate Loss\n",
    "            #print(rewards[index].shape)\n",
    "            advantages = rewards - state_values.detach()   \n",
    "            #print(rewards)\n",
    "            #print(ratios.shape,advantages.shape)\n",
    "            #advantages = advantages.reshape(temp_size,1)\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            #print(state_values)\n",
    "            #print(state_values.shape)\n",
    "            # final loss of clipped objective PPO\n",
    "            #print(rewards.shape,state_values.shape)\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "            #print(surr1,surr2,state_values,rewards,dist_entropy,loss)\n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            #self.optimizer2.step()\n",
    "        if self.entropy_c > 0.001:\n",
    "            self.entropy_c -= 0.0001\n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # clear buffer\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    \n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "   \n",
    "\n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_timestep = 1000     # update policy every n timesteps\n",
    "K_epochs = 40             # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0001      # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "random_seed = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ngsim_scenario(client: carla.Client, data_mode = \"train\") -> Scenario:\n",
    "    data_dir = os.environ.get(\"NGSIM_DIR\")\n",
    "    #data_dir = os.listdir('/home/surender/Downloads/NGSIM')\n",
    "    assert data_dir, \"Path to the directory with NGSIM dataset is required\"\n",
    "    ngsim_map = NGSimDatasets.list()\n",
    "    ngsim_dataset = ngsim_map[1]\n",
    "    client.load_world(ngsim_dataset.carla_map.level_path)\n",
    "    if data_mode == \"train\":\n",
    "        return NGSimLaneChangeScenario(\n",
    "            ngsim_dataset,\n",
    "            dataset_mode=DatasetMode.TRAIN,\n",
    "            data_dir=data_dir,\n",
    "            reward_type=RewardType.DENSE,\n",
    "            client=client,\n",
    "        )\n",
    "    else:\n",
    "        return NGSimLaneChangeScenario(\n",
    "            ngsim_dataset,\n",
    "            dataset_mode=DatasetMode.VALIDATION,\n",
    "            data_dir=data_dir,\n",
    "            reward_type=RewardType.DENSE,\n",
    "            client=client,\n",
    "        )\n",
    "def prepare_ego_vehicle(world: carla.World) -> carla.Actor:\n",
    "    car_blueprint = world.get_blueprint_library().find(\"vehicle.audi.a2\")\n",
    "\n",
    "    # This will allow external scripts like manual_control.py or no_rendering_mode.py\n",
    "    # from the official CARLA examples to take control over the ego agent\n",
    "    car_blueprint.set_attribute(\"role_name\", \"hero\")\n",
    "\n",
    "    # spawn points doesnt matter - scenario sets up position in reset\n",
    "    ego_vehicle = world.spawn_actor(\n",
    "        car_blueprint, carla.Transform(carla.Location(0, 0, 500), carla.Rotation())\n",
    "    )\n",
    "\n",
    "    assert ego_vehicle is not None, \"Ego vehicle could not be spawned\"\n",
    "\n",
    "    # Setup any car sensors you like, collect observations and then use them as input to your model\n",
    "    return ego_vehicle\n",
    "def cmd_carla():\n",
    "    os.system(\"DISPLAY= /home/surender/Downloads/carlaOld/CarlaUE4.sh -benchmark -fps=10 -quality-level=Low -opengl -Resx=4 -Resy=4 -NoVSync\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thp = threading.Thread(target = cmd_carla)\n",
    "thp.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"localhost\"\n",
    "port = 2000\n",
    "client = carla.Client(host,port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = prepare_ngsim_scenario(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = client.get_world()\n",
    "spectator = world.get_spectator()\n",
    "ego_vehicle = prepare_ego_vehicle(world)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scenario.reset(ego_vehicle)\n",
    "c = world.tick()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario._lane_change_instants[104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PPO(1,2,lr_actor,lr_critic,gamma,K_epochs,eps_clip,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward_list = []\n",
    "epoch_list = []\n",
    "step_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_front_back(y_origin):\n",
    "    d = scenario._ngsim_vehicles_in_carla._vehicle_by_vehicle_id\n",
    "    y_left = y_origin - 2\n",
    "    y_right = y_origin + 2\n",
    "    x = ego_vehicle.get_location().x\n",
    "    \n",
    "    \n",
    "    #Find closest front\n",
    "    min_dist = 10000\n",
    "    min_id = -1\n",
    "    \n",
    "    for i in d:\n",
    "        x_cor = d[i].get_location().x\n",
    "        y_cor = d[i].get_location().y\n",
    "        if x_cor > x and y_cor>y_left and y_cor<y_right:\n",
    "            diff = x_cor - x\n",
    "            if diff<min_dist:\n",
    "                min_dist = diff\n",
    "                min_id = i\n",
    "    #Find closest back\n",
    "    minb_dist = 10000\n",
    "    minb_id = -1\n",
    "    for i in d:\n",
    "        x_cor = d[i].get_location().x\n",
    "        y_cor = d[i].get_location().y\n",
    "        if x_cor < x and y_cor>y_left and y_cor<y_right:\n",
    "            diff =  x - x_cor\n",
    "            if diff<minb_dist:\n",
    "                minb_dist = diff\n",
    "                minb_id = i\n",
    "    return min_id,minb_id\n",
    "def prep_neighbours(current_frame):\n",
    "    d = scenario._ngsim_vehicles_in_carla._vehicle_by_vehicle_id\n",
    "    start_lane = find_front_back(scenario._start_lane_waypoint.transform.location.y)\n",
    "    target_lane = find_front_back(scenario._target_lane_waypoint.transform.location.y)\n",
    "    data = []\n",
    "    df = scenario._ngsim_recording._df_by_timeslot[scenario._lane_change.timeslot]\n",
    "    cf = df[df['Frame ID'] == current_frame]\n",
    "    sf = cf[cf[\"Lane Identification\"] == scenario._lane_change.lane_from]\n",
    "    srf = cf[cf[\"Lane Identification\"] == scenario._lane_change.lane_to]\n",
    "    default_spd = scenario._veh.speed/1.99\n",
    "    for i in (start_lane):\n",
    "        try:\n",
    "            x = d[i].get_location().x - ego_vehicle.get_location().x\n",
    "            y = d[i].get_location().y - ego_vehicle.get_location().y\n",
    "        except:\n",
    "            x = 0\n",
    "            y = 0 \n",
    "        #print(\"Start:\",sf[sf[\"Vehicle ID\"] == i][\"Vehicle Velocity\"])\n",
    "        try:\n",
    "            spd = float(sf[sf[\"Vehicle ID\"] == i][\"Vehicle Velocity\"])\n",
    "        except:\n",
    "            spd = default_spd\n",
    "        data.append(x/100)\n",
    "        data.append(y/100)\n",
    "        data.append(spd/130)\n",
    "    \n",
    "    for i in (target_lane):\n",
    "        try:\n",
    "            x = d[i].get_location().x - ego_vehicle.get_location().x\n",
    "            y = d[i].get_location().y - ego_vehicle.get_location().y\n",
    "        except:\n",
    "            x = 0\n",
    "            y = 0\n",
    "        #print(srf[srf[\"Vehicle ID\"] == i][\"Vehicle Velocity\"])\n",
    "        try:\n",
    "            spd = float(srf[srf[\"Vehicle ID\"] == i][\"Vehicle Velocity\"])\n",
    "        except:\n",
    "            spd = default_spd\n",
    "        data.append(x/100)\n",
    "        data.append(y/100)\n",
    "        data.append(spd/130)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birdview_producer = BirdViewProducer(\n",
    "    client,  # carla.Client\n",
    "    target_size=PixelDimensions(width=186, height=150),\n",
    "    pixels_per_meter=4\n",
    "    #crop_type=BirdViewCropType.FRONT_AREA_ONLY\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.buffer.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.load(\"NonCommBestModel.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pr = validate()\n",
    "p.buffer.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.policy_old.layer1.eval()\n",
    "p.policy_old.layer2.eval()\n",
    "p.policy_old.actor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"NonCommGif/information.pkl\",'wb')\n",
    "pickle.dump(infor,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ste = []\n",
    "for i in infor:\n",
    "    ste.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(ste)/len(ste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.6875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_points = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    p.buffer.clear()   \n",
    "    pr = validate()\n",
    "    p.buffer.clear()\n",
    "    val_points.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(val_points)/len(val_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(val_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(val_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"ValNonCommAllRandom.pkl\",'wb')\n",
    "pickle.dump(val_points,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "section = 5\n",
    "epochs = 5000\n",
    "freq = 5\n",
    "freq_n = 3\n",
    "update_freq = 100\n",
    "decay_freq = 1000\n",
    "decay_c = 1000\n",
    "#val = 0\n",
    "min_r_avg = -1\n",
    "steer_w = 1\n",
    "min_steer_w = 0\n",
    "steer_decay = 0.1\n",
    "mn=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "last_epoch = -1\n",
    "\n",
    "victory = 0\n",
    "victory_buffer = 0\n",
    "buffer_limit = 5000\n",
    "for epoch in range(1601,epochs):\n",
    "    step = 0\n",
    "    \n",
    "    #scenario = prepare_ngsim_scenario(client)\n",
    "    #world = client.get_world()\n",
    "    #ego_vehicle = prepare_ego_vehicle(world)\n",
    "    #scenario.reset(ego_vehicle)\n",
    "    #c = world.tick()\n",
    "    #print(len(list(world.get_actors())))\n",
    "    #del ego_vehicle\n",
    "    ids = list(world.get_actors())[0]\n",
    "    carla.command.DestroyActor(ids)\n",
    "    '''\n",
    "    for i in list(world.get_actors()):\n",
    "        if type(i) != carla.libcarla.Vehicle:\n",
    "            ids = i.id\n",
    "            carla.command.DestroyActor(ids)\n",
    "    '''\n",
    "    world = client.get_world()\n",
    "    #spectator = world.get_spectator()\n",
    "    ego_vehicle = prepare_ego_vehicle(world)\n",
    "    \n",
    "    #scenario.close()\n",
    "    #scenario = prepare_ngsim_scenario(client)\n",
    "    \n",
    "    #time.sleep(3)\n",
    "    #world = client.get_world()\n",
    "    #spectator = world.get_spectator()\n",
    "    #client.reload_world()\n",
    "    \n",
    "    #ego_vehicle = prepare_ego_vehicle(world)\n",
    "    \n",
    "    scenario.reset(ego_vehicle)\n",
    "    c = world.tick()\n",
    "    \n",
    "    ego_vehicle.apply_control(carla.VehicleControl(manual_gear_shift=True, gear=4))\n",
    "    #ego_vehicle.apply_control(carla.VehicleControl(throttle=1.0, brake=1.0))\n",
    "    c = world.tick()\n",
    "    #ego_vehicle.apply_control(carla.VehicleControl(manual_gear_shift=False))\n",
    "    #time.sleep(4)\n",
    "    #ego_vehicle.apply_control(carla.VehicleControl(brake=0.0))\n",
    "    try:\n",
    "        cmd,reward,dp,_ = scenario.step(ego_vehicle)\n",
    "    except:\n",
    "        continue\n",
    "    c = world.tick()\n",
    "    \n",
    "    #way = scenario._target_lane_waypoint.transform\n",
    "    speed = scenario._veh.speed/1.99\n",
    "    #c = world.tick()\n",
    "    done = False\n",
    "    total_r = 0\n",
    "    val = 0\n",
    "    way = scenario._target_lane_waypoint.transform\n",
    "    target_boundary = scenario._target_lane_waypoint.transform\n",
    "    ##left_y = target_boundary.location.y - 5\n",
    "    #right_y = target_boundary.location.y + 5\n",
    "    #initial_x \n",
    "    #way = way_cal(ego_vehicle,val)\n",
    "    #print(scenario._sampled_dataset_excerpt_info)\n",
    "    t_clip_n = 0.0\n",
    "    t_clip_p = 1.0\n",
    "    \n",
    "    s_clip_n = -1.0\n",
    "    s_clip_p = 1.0\n",
    "    cmd_buffer = [0]\n",
    "    yaw_buffer = [0]\n",
    "    \n",
    "    steer = 0\n",
    "    steer_aug = 0.1\n",
    "    prep_limit = 10*1\n",
    "    pid = VehiclePIDController(ego_vehicle,0.7)\n",
    "    lock = 0\n",
    "    if victory_buffer > buffer_limit:\n",
    "        section+=1\n",
    "        victory_buffer = 0\n",
    "        buffer_limit += buffer_limit\n",
    "    while not done:\n",
    "        '''\n",
    "        while True:\n",
    "            #print(current_frame,c)\n",
    "            if current_frame >= c:\n",
    "                #print(current_frame,c)\n",
    "                break\n",
    "        '''\n",
    "        #speed = scenario._veh.speed/1.99\n",
    "        way = scenario._target_lane_waypoint.transform\n",
    "        way.location.x = ego_vehicle.get_location().x + 30\n",
    "        #safe = check_safety(way)\n",
    "        \n",
    "        \n",
    "        #print(safe)\n",
    "        if step < prep_limit:\n",
    "            speed = scenario._veh.speed/1.99\n",
    "            k = pid.run_step(speed,way)\n",
    "            #print(k.throttle)\n",
    "            birdview = birdview_producer.produce(\n",
    "                agent_vehicle=ego_vehicle  # carla.Actor (spawned vehicle)\n",
    "                )\n",
    "            ego_vehicle.apply_control(carla.VehicleControl(throttle=k.throttle, steer=np.clip(0.0,-1.0,1.0),brake=k.brake))\n",
    "        else:\n",
    "            if lock == 0:\n",
    "                pid = VehiclePIDController(ego_vehicle,0.7)\n",
    "                lock = 1\n",
    "            birdview = birdview_producer.produce(\n",
    "                agent_vehicle=ego_vehicle  # carla.Actor (spawned vehicle)\n",
    "                )\n",
    "            #a = birdview[0].reshape(1,186,150)\n",
    "            #a = np.append(a,birdview[1].reshape(1,186,150),axis=0)\n",
    "            #a = np.append(a,birdview[2].reshape(1,186,150),axis=0)\n",
    "            #a = np.append(a,birdview[3].reshape(1,186,150),axis=0)\n",
    "            #a# = np.append(a,birdview[4].reshape(1,186,150),axis=0)\n",
    "            #a = np.append(a,birdview[4].reshape(1,224,224),axis=0)\n",
    "            #rgb = BirdViewProducer.as_rgb(birdview)/255.\n",
    "            #in_data = a.reshape(1,5,186,150)\n",
    "            '''\n",
    "            for i in range(3):\n",
    "                rgb[:,:,i] = (rgb[:,:,i] - mn[i])/std[i]\n",
    "            '''\n",
    "            #in_data = rgb.reshape(1,3,224,224)\n",
    "            '''\n",
    "            with torch.no_grad():\n",
    "                in_data = torch.FloatTensor(in_data).to(device)\n",
    "                in_data = mobilenet.features(in_data)\n",
    "                in_data = nn.AvgPool2d(7,7)(in_data)\n",
    "                in_data = nn.Flatten()(in_data)\n",
    "                in_data = in_data.detach().cpu().numpy()\n",
    "            '''\n",
    "            \n",
    "            #in_data = mdl(in_data).numpy()\n",
    "            #in_data = in_data.reshape(in_data.shape[0],62720)\n",
    "            #in_data = in_data.reshape((1,5,186,150))\n",
    "            #in_data = input_data.reshape((1,3,320,320))\n",
    "            #print(in_data.shape)\n",
    "            if val == 0 or val == 1:\n",
    "                inps= [0,1,0,speed/130]\n",
    "            if val == 2 or val == 5:\n",
    "                inps = [0,0,1,speed/130]\n",
    "            if val == 4 or val ==3:\n",
    "                inps = [1,0,0,speed/130]\n",
    "            #inps = np.array(inps)\n",
    "            data = prep_neighbours(_[\"ngsim_dataset\"][\"frame\"])\n",
    "            data = data + inps\n",
    "            inputs = np.array(data).reshape(1,1,16)\n",
    "            #ap = np.concatenate((in_data[0],inps)).reshape(1,1283)\n",
    "\n",
    "            action = p.select_action(inputs)\n",
    "            #print(action)\n",
    "            #print(action)\n",
    "            #action[0] = action[0]*2 - 1\n",
    "            #action[1] = action[1]*120\n",
    "            '''\n",
    "            if (val == 0  or val ==1):\n",
    "                s_clip_n = -0.15\n",
    "                s_clip_p = 0.15\n",
    "                t_clip_n = 0.4\n",
    "                t_clip_p = 1.0\n",
    "\n",
    "            if (val == 2 or val == 5):\n",
    "                s_clip_n = 0.25\n",
    "                s_clip_p = 0.8\n",
    "                t_clip_n = 0.0\n",
    "                t_clip_p = 0.4\n",
    "\n",
    "            if (val == 3 or val == 4):\n",
    "                s_clip_n = -0.8\n",
    "                s_clip_p = -0.25\n",
    "                t_clip_n = 0.0\n",
    "                t_clip_p = 0.4\n",
    "            '''\n",
    "\n",
    "\n",
    "            #t_clip_n = 0.0\n",
    "            #t_clip_p = 0.7\n",
    "\n",
    "            #s_clip_n = -0.6\n",
    "            #s_clip_p = 0.6  \n",
    "            #d_action = action_space[action]\n",
    "            #steer_s = action[0]*0.1\n",
    "            #steer += steer_s\n",
    "            '''\n",
    "            if action[0]<0:\n",
    "                steer -= steer_aug\n",
    "            elif action[0]>0:\n",
    "                steer += steer_aug\n",
    "            \n",
    "            if steer < -0.1:\n",
    "                steer = -0.1\n",
    "            if steer > 0.1:\n",
    "                steer = 0.1\n",
    "            '''\n",
    "            #speed = action[0]*80+20\n",
    "            #speed = action[1]\n",
    "            #speed = math.sqrt(speed**2)\n",
    "            #_speed = np.clip(action[0], -1,1)\n",
    "            #speed = ((_speed + 1)/2)*50 + 10\n",
    "            \n",
    "            \n",
    "            brake = 0\n",
    "            throttle = 0\n",
    "            steer = 0\n",
    "            steer = 0\n",
    "            \n",
    "            if action == 0:\n",
    "                x = 15\n",
    "                #speed = speed - 1\n",
    "            if action == 1:\n",
    "                x = 30\n",
    "                #speed = speed - 1\n",
    "            if action == 2:\n",
    "                x = 50 \n",
    "                #speed = speed - 1\n",
    "            if action == 3:\n",
    "                x = 75\n",
    "                #speed = speed - 1\n",
    "            if action == 4:\n",
    "                x = 15\n",
    "                speed = speed - 1\n",
    "            if action == 5:\n",
    "                x = 30\n",
    "                speed = speed - 1\n",
    "            if action == 6:\n",
    "                x = 50\n",
    "                speed = speed - 1\n",
    "            if action == 7:\n",
    "                x = 75\n",
    "                speed = speed - 1\n",
    "\n",
    "            if action == 8:\n",
    "                x = 15\n",
    "                speed = speed + 1\n",
    "            if action == 9:\n",
    "                x = 30\n",
    "                speed = speed + 1\n",
    "            if action == 10:\n",
    "                x = 50\n",
    "                speed = speed + 1\n",
    "            if action == 11:\n",
    "                x = 75\n",
    "                speed = speed + 1\n",
    "            '''\n",
    "            if action == 0:\n",
    "                x = 5\n",
    "                speed = speed - 1\n",
    "            if action == 1:\n",
    "                x = 15\n",
    "                speed = speed - 1\n",
    "            if action == 2:\n",
    "                x = 25 \n",
    "                speed = speed - 1\n",
    "            if action == 3:\n",
    "                x = 35\n",
    "                speed = speed - 1\n",
    "            if action == 4:\n",
    "                x = 45\n",
    "                speed = speed - 1\n",
    "            if action == 5:\n",
    "                x = 55\n",
    "                speed = speed - 1\n",
    "            if action == 6:\n",
    "                x = 65\n",
    "                speed = speed - 1\n",
    "            if action == 7:\n",
    "                x = 75\n",
    "                speed = speed - 1\n",
    "            if action == 8:\n",
    "                x = 85\n",
    "                speed = speed - 1\n",
    "            if action == 9:\n",
    "                x = 95\n",
    "                speed = speed -1\n",
    "            if action == 10:\n",
    "                x = 95\n",
    "                speed = speed + 1\n",
    "            if action == 11:\n",
    "                x = 85\n",
    "                speed = speed + 1\n",
    "            if action == 12:\n",
    "                x = 75\n",
    "                speed = speed + 1\n",
    "            if action == 13:\n",
    "                x = 65\n",
    "                speed = speed + 1\n",
    "            if action == 14:\n",
    "                x = 55\n",
    "                speed = speed + 1\n",
    "            if action == 15:\n",
    "                x = 45\n",
    "                speed = speed + 1\n",
    "            if action == 16:\n",
    "                x = 35\n",
    "                speed = speed + 1\n",
    "            if action == 17:\n",
    "                x = 25\n",
    "                speed = speed + 1\n",
    "            if action == 18:\n",
    "                x = 15\n",
    "                speed = speed + 1\n",
    "            if action == 19:\n",
    "                x = 5\n",
    "                speed = speed + 1\n",
    "            '''\n",
    "            #x = (action[0] + 1)/2\n",
    "            #x = x*100\n",
    "            #y = (action[1] + 1)/2\n",
    "            #y = y*130\n",
    "            way.location.x =  ego_vehicle.get_location().x + x\n",
    "            #speed = y\n",
    "            #ay.location.x =  ego_vehicle.get_location().x + x\n",
    "            #way.location.y = ego_vehicle.get_location().y + y\n",
    "            '''\n",
    "            brake = 0\n",
    "            throttle = 0\n",
    "            if action[0] <0:\n",
    "                brake = action[0]\n",
    "                throttle = 0\n",
    "            else:\n",
    "                throttle = action[0]\n",
    "                brake = 0\n",
    "            '''\n",
    "            '''\n",
    "            if epoch > 400:\n",
    "                if (val == 0  or val ==1):\n",
    "                    s_clip_n = -0.15\n",
    "                    s_clip_p = 0.15\n",
    "                    t_clip_n = 0.4\n",
    "                    t_clip_p = 1.0\n",
    "\n",
    "                if (val == 2 or val == 5):\n",
    "                    s_clip_n = 0.25\n",
    "                    s_clip_p = 0.6\n",
    "                    t_clip_n = 0.0\n",
    "                    t_clip_p = 0.4\n",
    "\n",
    "                if (val == 3 or val == 4):\n",
    "                    s_clip_n = -0.6\n",
    "                    s_clip_p = -0.25\n",
    "                    t_clip_n = 0.0\n",
    "                    t_clip_p = 0.4\n",
    "            '''\n",
    "            #if epoch < 20:\n",
    "            #print(throttle,steer,brake)\n",
    "            #pid = VehiclePIDController(ego_vehicle)\n",
    "\n",
    "            k = pid.run_step(speed,way)\n",
    "            #throttle = k.throttle\n",
    "            #brake = k.brake\n",
    "\n",
    "            avg_steer = k.steer \n",
    "            brake = k.brake\n",
    "            throttle = k.throttle\n",
    "            #print(\"SteerDiff:\",abs(steer-k.steer),k.steer,steer)\n",
    "            #print(throttle,steer,brake)\n",
    "            #avg_steer= k.steer\n",
    "            ego_vehicle.apply_control(carla.VehicleControl(throttle=throttle, steer=np.clip(avg_steer,-1.0,1.0),brake=brake))\n",
    "\n",
    "        #ego_vehicle.apply_control(carla.VehicleControl(throttle=np.clip(throttle, t_clip_n, t_clip_p), steer=np.clip(action[1], s_clip_n, s_clip_p)))#,brake=np.clip(brake, 0.0, 1.0)))\n",
    "        \n",
    "        \n",
    "        #print(steer,speed)\n",
    "        try:\n",
    "            cmd, reward, done, _ = scenario.step(ego_vehicle)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"ERROR\")\n",
    "            reward = -1\n",
    "            done = True\n",
    "        #reward = reward/2\n",
    "        \n",
    "        '''\n",
    "        if reward < 0 :\n",
    "            reward = -0.1\n",
    "        '''\n",
    "        \n",
    "        val = cmd.value\n",
    "        #way = way_cal(ego_vehicle,val)\n",
    "        cmd_buffer.append(val)\n",
    "        yaw_buffer.append(ego_vehicle.get_transform().rotation.yaw)\n",
    "        if section == 0:\n",
    "            if len(cmd_buffer) > 10:\n",
    "                if _['on_target_lane']:\n",
    "                    reward = 1 \n",
    "                    done = True\n",
    "        if section == 1:\n",
    "            if len(cmd_buffer) > 10:\n",
    "                if sum(cmd_buffer[-10:]) == 0 and _['on_target_lane'] and total_r>=0.5:\n",
    "                    reward = 1 \n",
    "                    done = True\n",
    "        if section == 2:\n",
    "            if len(cmd_buffer) > 10:\n",
    "                if sum(cmd_buffer[-10:]) == 0 and _['on_target_lane'] and sum([abs(x) for x in yaw_buffer[-10:]])/10<=10 and total_r>=0.5:\n",
    "                    reward = 1 \n",
    "                    done = True\n",
    "        if section==3:\n",
    "            if len(cmd_buffer) > 10:\n",
    "                if sum(cmd_buffer[-10:]) == 0 and _['on_target_lane'] and sum([abs(x) for x in yaw_buffer[-10:]])/10<=10 and total_r>=0.6:\n",
    "                    reward = 1 \n",
    "                    done = True\n",
    "        if section==4:\n",
    "            if len(cmd_buffer) > 10:\n",
    "                if sum(cmd_buffer[-10:]) == 0 and _['on_target_lane'] and sum([abs(x) for x in yaw_buffer[-10:]])/10<=5 and total_r>=0.8:\n",
    "                    reward = 1 \n",
    "                    done = True\n",
    "        if section>=5:\n",
    "            pass\n",
    "        if total_r >=0.9:\n",
    "            print(\"Close to win\")\n",
    "        if reward >= 1:\n",
    "            victory_buffer+=1\n",
    "            print(\"Its a win\")\n",
    "            \n",
    "        \n",
    "            #p.save(\"WinModel\"+str(epoch)+\".mdl\")\n",
    "        #way = _[\"scenario_data\"][\"original_veh_transform\"]\n",
    "        #way = scenario._target_lane_waypoint.transform  \n",
    "        \n",
    "        \n",
    "        #print(action[0],action[1])\n",
    "        #print(done)\n",
    "        #if done:\n",
    "        #    print(_)\n",
    "        #print(_)\n",
    "        '''\n",
    "        if (val == 0  or val ==1):\n",
    "            if action[0] > 0.0:\n",
    "                reward += 0.01\n",
    "            if action[1] < 0.1 and action[1]> -0.1:\n",
    "                reward += 0.01\n",
    "                \n",
    "\n",
    "        if (val == 2 or val == 5):\n",
    "            if action[0] <0.1:\n",
    "                reward += 0.01\n",
    "            if action[1] > 0.0:\n",
    "                reward += 0.01\n",
    "\n",
    "        if (val == 3 or val == 4):\n",
    "            if action[0] < 0.1:\n",
    "                reward += 0.01\n",
    "            if action[1] < 0.0:\n",
    "                reward += 0.01\n",
    "        '''\n",
    "        #reward += step\n",
    "        '''\n",
    "        v = ego_vehicle.get_velocity()\n",
    "        kmh = int(3.6 * math.sqrt(v.x**2 + v.y**2 + v.z**2))\n",
    "        \n",
    "        if kmh < 60 & kmh > 0.2:\n",
    "            #done = False\n",
    "            reward += 1 #-1\n",
    "            # Reward lighter steering when moving\n",
    "            if np.abs(action[1]) < 0.3:\n",
    "                reward += 1\n",
    "            elif np.abs(action[1]) > 0.5 and np.abs(action[1]) < 0.9:\n",
    "                reward -= 0.1\n",
    "            elif np.abs(action[1]) >= 0.9:\n",
    "                reward -= 0.2\n",
    "        elif kmh < 0.2:\n",
    "            reward -= 0.1\n",
    "        else:\n",
    "            #print(\"Maybe never\")\n",
    "            reward += 0.01\n",
    "            if np.abs(action[1]) < 0.3:\n",
    "                reward += 0.12\n",
    "            # Reduce score for heavy steering\n",
    "            if np.abs(action[1]) > 0.5 and np.abs(action[1]) < 0.9:\n",
    "                reward -= 0.17\n",
    "            elif np.abs(action[1]) >= 0.9:\n",
    "                reward -= 0.21\n",
    "        '''\n",
    "        \n",
    "        rgb = BirdViewProducer.as_rgb(birdview)\n",
    "        cv2.imshow('Frame',rgb)\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "        if step < prep_limit:\n",
    "            pass\n",
    "        else:\n",
    "            #print(\"HI\")\n",
    "            p.buffer.rewards.append(reward)\n",
    "            p.buffer.is_terminals.append(done)\n",
    "        \n",
    "        total_r += reward\n",
    "        step += 1\n",
    "        \n",
    "        '''\n",
    "        if step % freq ==0 :\n",
    "            print(step)\n",
    "            p.update()\n",
    "        \n",
    "        if step % freq_n == 0:\n",
    "            p.decay_action_std(0.05,0.1)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        world.tick()\n",
    "    #total_r += step/10\n",
    "    '''\n",
    "    if epoch > 50:\n",
    "        \n",
    "        if sum(total_reward_list[-50:])/len(total_reward_list[-50:]) > min_r_avg or sum(step_list) > decay_freq:\n",
    "            decay_freq += decay_c\n",
    "            #decay_c -= 50\n",
    "            #print(\"Decaying:\",p.action_std)\n",
    "            p.decay_action_std(0.01,0.1)\n",
    "            \n",
    "            min_r_avg = sum(total_reward_list[-50:])/len(total_reward_list[-50:])\n",
    "    '''\n",
    "    if epoch%update_freq ==0 and epoch != 0 and len(p.buffer.states)>1:\n",
    "            print(k.steer)\n",
    "            print(\"Update with batches:\",len(p.buffer.states))\n",
    "            p.update()\n",
    "            vs = validate()\n",
    "            val_points.append(vs)\n",
    "            print(vs)\n",
    "            p.buffer.clear()\n",
    "            #update_freq = 10\n",
    "            if steer_w > min_steer_w +0.1:\n",
    "                steer_w -= steer_decay\n",
    "    '''\n",
    "    else:\n",
    "            print(total_r,epoch,step)\n",
    "            total_reward_list.append(total_r)\n",
    "            epoch_list.append(epoch)\n",
    "            step_list.append(step)\n",
    "            continue\n",
    "    #except Exception as e:\n",
    "        #print(\"Error:\",e)\n",
    "        #pass\n",
    "    '''\n",
    "    if step <10 and epoch != 0 and epoch!=last_epoch+1:\n",
    "        last_epoch = epoch\n",
    "        scenario = prepare_ngsim_scenario(client)\n",
    "        world = client.get_world()\n",
    "        ego_vehicle = prepare_ego_vehicle(world)\n",
    "        scenario.reset(ego_vehicle)\n",
    "        world.tick()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(total_r,epoch,step,\"Section:\",section,\"V_Buffer:\",victory_buffer)\n",
    "    total_reward_list.append(total_r)\n",
    "    epoch_list.append(epoch)\n",
    "    step_list.append(step)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_rgb = {}\n",
    "infor = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate():\n",
    "    global max_score\n",
    "    val_success = []\n",
    "    try:\n",
    "        scenario.close()\n",
    "        #self.scenario_val.close()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    scenario = prepare_ngsim_scenario(client,\"Val\")\n",
    "    world = client.get_world()\n",
    "    ego_vehicle = prepare_ego_vehicle(world)\n",
    "    birdview_producer = BirdViewProducer(\n",
    "            client,  # carla.Client\n",
    "            target_size=PixelDimensions(width=186, height=150),\n",
    "            pixels_per_meter=4\n",
    "            #crop_type=BirdViewCropType.FRONT_AREA_ONLY\n",
    "    )\n",
    "    \n",
    "    #self.scenario_val.reset(self.ego_vehicle)\n",
    "    scenario.reset(ego_vehicle,0)\n",
    "    world.tick()\n",
    "    val_count = 0\n",
    "    for i in range(100):\n",
    "        temp_rgb = []\n",
    "        t_clip_n = 0.0\n",
    "        t_clip_p = 1.0\n",
    "\n",
    "        s_clip_n = -1.0\n",
    "        s_clip_p = 1.0\n",
    "        #torch.cuda.empty_cache()\n",
    "        step = 0\n",
    "        #del self.ego_vehicle\n",
    "        #self.scenario = prepare_ngsim_scenario(self.client,\"Val\")\n",
    "        ids = list(world.get_actors())[0].id\n",
    "        carla.command.DestroyActor(ids)\n",
    "        world = client.get_world()\n",
    "        #self.spectator = self.world.get_spectator()\n",
    "        ego_vehicle = prepare_ego_vehicle(world)\n",
    "        \n",
    "        scenario.reset(ego_vehicle,i)\n",
    "        c = world.tick()\n",
    "        '''\n",
    "        cam_bp = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "        cam_bp.set_attribute(\"image_size_x\",str(320))\n",
    "        cam_bp.set_attribute(\"image_size_y\",str(320))\n",
    "        cam_bp.set_attribute(\"fov\",str(100))\n",
    "        cam_location = carla.Location(-5,0,4)\n",
    "        cam_rotation = carla.Rotation(0,0,0)\n",
    "        cam_transform = carla.Transform(cam_location,cam_rotation)\n",
    "        ego_front_cam = world.spawn_actor(cam_bp,cam_transform,attach_to=ego_vehicle, attachment_type=carla.AttachmentType.Rigid)\n",
    "        #self.rgb_front_listener = ego_cam\n",
    "        ego_front_cam.listen(lambda image: check_img(image,i))\n",
    "        '''\n",
    "        ego_vehicle.apply_control(carla.VehicleControl(manual_gear_shift=True, gear=4))\n",
    "        c = world.tick()\n",
    "        try:\n",
    "            cmd,reward,dp,_ = scenario.step(ego_vehicle)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        c = world.tick()\n",
    "        speed = scenario._veh.speed/1.99\n",
    "\n",
    "        '''\n",
    "        except Exception as e:\n",
    "            print(\"Error in val init:\",e)\n",
    "            val_success.append(0)\n",
    "            self.load_carla()\n",
    "            continue\n",
    "        '''\n",
    "\n",
    "        done = False\n",
    "        reward = 0\n",
    "        val = 0\n",
    "        #way = self.way_cal(self.ego_vehicle,val)\n",
    "        way = scenario._target_lane_waypoint.transform\n",
    "        cmd_buffer = [0]\n",
    "        yaw_buffer = [0]\n",
    "        total_rew = 0\n",
    "        target_buffer = [False]\n",
    "        steer = 0\n",
    "        steer_aug = 0.05\n",
    "        prep_limit = 10*1\n",
    "        pid = VehiclePIDController(ego_vehicle,0.7)\n",
    "        lock = 0\n",
    "        fin = 0\n",
    "        while not done:\n",
    "                birdview = birdview_producer.produce(\n",
    "                agent_vehicle=ego_vehicle  # carla.Actor (spawned vehicle)\n",
    "                )\n",
    "                #speed = scenario._veh.speed/1.99\n",
    "                way = scenario._target_lane_waypoint.transform\n",
    "                way.location.x = ego_vehicle.get_location().x + 40\n",
    "                #safe = self.check_safety(way)\n",
    "                '''\n",
    "                while True:\n",
    "                    #print(current_frame,c)\n",
    "                    if current_frame >= c:\n",
    "                        #print(current_frame,c)\n",
    "                        break\n",
    "                '''\n",
    "                if step < prep_limit:\n",
    "                    speed = scenario._veh.speed/1.99\n",
    "                    k = pid.run_step(speed,way)\n",
    "                    #print(k.throttle)\n",
    "                    ego_vehicle.apply_control(carla.VehicleControl(throttle=k.throttle, steer=np.clip(0.0,-1.0,1.0),brake=k.brake))\n",
    "                else:\n",
    "                    if lock == 0:\n",
    "                        pid = VehiclePIDController(ego_vehicle,0.7)\n",
    "                        lock = 1\n",
    "                    #birdview = self.birdview_producer.produce(\n",
    "                    #    agent_vehicle=self.ego_vehicle  # carla.Actor (spawned vehicle)\n",
    "                    #    )\n",
    "                    #a = birdview[0].reshape(1,50,150)\n",
    "                    #a = np.append(a,birdview[1].reshape(1,50,150),axis=0)\n",
    "                    # = np.append(a,birdview[2].reshape(1,50,150),axis=0)\n",
    "                    ##a = np.append(a,birdview[3].reshape(1,50,150),axis=0)\n",
    "                    #a = np.append(a,birdview[4].reshape(1,50,150),axis=0)\n",
    "                    #a = np.append(a,birdview[4].reshape(1,224,224),axis=0)\n",
    "                    '''\n",
    "                    rgb = BirdViewProducer.as_rgb(birdview)/255.\n",
    "                    for i in range(3):\n",
    "                        rgb[:,:,i] = (rgb[:,:,i] - self.mn[i])/self.std[i]\n",
    "                    #in_data = a.reshape(1,5,186,150)\n",
    "                    in_data = rgb.reshape(1,3,224,224)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        in_data = torch.FloatTensor(in_data).to(device)\n",
    "                        in_data = self.mobilenet.features(in_data)\n",
    "                        in_data = nn.AvgPool2d(7,7)(in_data)\n",
    "                        in_data = nn.Flatten()(in_data)\n",
    "                        in_data = in_data.detach().cpu().numpy()\n",
    "                    '''\n",
    "                    #in_data = a.reshape(1,5,50,150)\n",
    "                    if val == 0 or val == 1:\n",
    "                        inps= [0,1,0,speed/130]\n",
    "                    if val == 2 or val == 5:\n",
    "                        inps = [0,0,1,speed/130]\n",
    "                    if val == 4 or val ==3:\n",
    "                        inps = [1,0,0,speed/130]\n",
    "                    #inps = np.array(inps)\n",
    "                    #ap = np.concatenate((in_data[0],inps)).reshape(1,1283)\n",
    "                    #inputs = [in_data,np.array(inps).reshape(1,4)]\n",
    "                    data = prep_neighbours(_[\"ngsim_dataset\"][\"frame\"])\n",
    "                    data = data + inps\n",
    "                    #inps = np.array(inps)\n",
    "                    #ap = np.concatenate((in_data[0],inps)).reshape(1,1283)\n",
    "                    #inputs = [in_data,np.array(inps).reshape(1,4)]\n",
    "                    inputs = np.array(data).reshape(1,1,16)\n",
    "                    with torch.no_grad():\n",
    "                        action = p.select_action(inputs,True)\n",
    "                    #action = action * 2 - 1\n",
    "                    steer = 0\n",
    "                    #x = (action[0] + 1)/2\n",
    "                    #x = x*100\n",
    "                    #y = action[1]*5\n",
    "                    #y = (action[1] +1)/2\n",
    "                    #y = y*130\n",
    "                    if action == 0:\n",
    "                        x = 15\n",
    "                        #speed = speed - 1\n",
    "                    if action == 1:\n",
    "                        x = 30\n",
    "                        #speed = speed - 1\n",
    "                    if action == 2:\n",
    "                        x = 50 \n",
    "                        #speed = speed - 1\n",
    "                    if action == 3:\n",
    "                        x = 75\n",
    "                        #speed = speed - 1\n",
    "                    if action == 4:\n",
    "                        x = 15\n",
    "                        speed = speed - 1\n",
    "                    if action == 5:\n",
    "                        x = 30\n",
    "                        speed = speed - 1\n",
    "                    if action == 6:\n",
    "                        x = 50\n",
    "                        speed = speed - 1\n",
    "                    if action == 7:\n",
    "                        x = 75\n",
    "                        speed = speed - 1\n",
    "                    \n",
    "                    if action == 8:\n",
    "                        x = 15\n",
    "                        speed = speed + 1\n",
    "                    if action == 9:\n",
    "                        x = 30\n",
    "                        speed = speed + 1\n",
    "                    if action == 10:\n",
    "                        x = 50\n",
    "                        speed = speed + 1\n",
    "                    if action == 11:\n",
    "                        x = 75\n",
    "                        speed = speed + 1\n",
    "                    \n",
    "                    '''\n",
    "                    speed_var = 0.5\n",
    "                    if action == 0:\n",
    "                        x = 5\n",
    "                        speed = speed - speed_var\n",
    "                    if action == 1:\n",
    "                        x = 15\n",
    "                        speed = speed - speed_var\n",
    "                    if action == 2:\n",
    "                        x = 25 \n",
    "                        speed = speed - speed_var\n",
    "                    if action == 3:\n",
    "                        x = 35\n",
    "                        speed = speed - speed_var\n",
    "                    if action == 4:\n",
    "                        x = 45\n",
    "                        speed = speed - speed_var\n",
    "                    if action == 5:\n",
    "                        x = 55\n",
    "                        speed = speed - speed_var\n",
    "                    if action == 6:\n",
    "                        x = 65\n",
    "                        speed = speed - speed_var\n",
    "                    if action == 7:\n",
    "                        x = 75\n",
    "                        speed = speed - speed_var\n",
    "                    if action == 8:\n",
    "                        x = 85\n",
    "                        speed = speed - speed_var\n",
    "                    if action == 9:\n",
    "                        x = 95\n",
    "                        speed = speed -speed_var\n",
    "                    if action == 10:\n",
    "                        x = 95\n",
    "                        speed = speed + speed_var\n",
    "                    if action == 11:\n",
    "                        x = 85\n",
    "                        speed = speed + speed_var\n",
    "                    if action == 12:\n",
    "                        x = 75\n",
    "                        speed = speed + speed_var\n",
    "                    if action == 13:\n",
    "                        x = 65\n",
    "                        speed = speed + speed_var\n",
    "                    if action == 14:\n",
    "                        x = 55\n",
    "                        speed = speed + speed_var\n",
    "                    if action == 15:\n",
    "                        x = 45\n",
    "                        speed = speed + speed_var\n",
    "                    if action == 16:\n",
    "                        x = 35\n",
    "                        speed = speed + speed_var\n",
    "                    if action == 17:\n",
    "                        x = 25\n",
    "                        speed = speed + speed_var\n",
    "                    if action == 18:\n",
    "                        x = 15\n",
    "                        speed = speed + speed_var\n",
    "                    if action == 19:\n",
    "                        x = 5\n",
    "                        speed = speed + speed_var\n",
    "\n",
    "                    '''\n",
    "                    \n",
    "                    way.location.x =  ego_vehicle.get_location().x + x\n",
    "                    #way.location.y = self.ego_vehicle.get_location().y + y\n",
    "                    #speed = y\n",
    "                    brake = 0\n",
    "                    throttle = 0\n",
    "                    #_speed = np.clip(action[0], -1,1)\n",
    "                    #speed = ((_speed + 1)/2)*50 + 10\n",
    "\n",
    "\n",
    "\n",
    "                    #pid = VehiclePIDController(self.ego_vehicle)\n",
    "                    k = pid.run_step(speed,way)\n",
    "                    throttle = k.throttle\n",
    "                    brake = k.brake\n",
    "\n",
    "                    avg_steer = k.steer\n",
    "                    ego_vehicle.apply_control(carla.VehicleControl(throttle=throttle, steer=np.clip(avg_steer, s_clip_n, s_clip_p),brake=brake))\n",
    "\n",
    "                #ego_vehicle.apply_control(carla.VehicleControl(throttle=np.clip(throttle, t_clip_n, t_clip_p), steer=np.clip(action[1], s_clip_n, s_clip_p)))#,brake=np.clip(brake, 0.0, 1.0)))\n",
    "\n",
    "\n",
    "                try:\n",
    "\n",
    "                    cmd, reward, done, _ = scenario.step(ego_vehicle)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    break\n",
    "                '''\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "                if reward < 0 :\n",
    "                    reward = -0.1\n",
    "                '''\n",
    "                '''\n",
    "                if reward <= -1:\n",
    "                    reward = 0\n",
    "                '''\n",
    "\n",
    "                #print(reward, cmd)\n",
    "\n",
    "                val = cmd.value\n",
    "                cmd_buffer.append(val)\n",
    "                yaw_buffer.append(ego_vehicle.get_transform().rotation.yaw)\n",
    "                target_buffer.append(_['on_target_lane'])\n",
    "                #fin = 0\n",
    "                if len(cmd_buffer) > 10:\n",
    "                    if sum(cmd_buffer[-10:]) == 0 and _['on_target_lane'] and sum([abs(x) for x in yaw_buffer[-10:]])/10<=10 and total_rew>=0.7:\n",
    "                        reward = 1\n",
    "                        done = True\n",
    "                        pass\n",
    "                '''\n",
    "                if len(cmd_buffer) > 10:\n",
    "                    if _['on_target_lane'] and all(item == True for item in target_buffer[-10:]):\n",
    "                        reward = 1 + random.random()\n",
    "                        done = True\n",
    "\n",
    "                '''\n",
    "                #rgb = BirdViewProducer.as_rgb(birdview)\n",
    "                #temp_rgb.append(rgb)\n",
    "                total_rew += reward\n",
    "                #total_rew += reward\n",
    "                #way = self.way_cal(self.ego_vehicle,val)\n",
    "                #way = scenario._target_lane_waypoint.transform \n",
    "                c = world.tick()\n",
    "                step+=1\n",
    "        #iter_rgb[i] = temp_rgb\n",
    "        if step>1:\n",
    "            val_count+=1\n",
    "        if total_rew >= 0.0:\n",
    "            #print(\"Success!\")\n",
    "            #val_reward.append(reward)\n",
    "            #val_count += 1\n",
    "            \n",
    "            if fin == 1:\n",
    "                total_rew += 1.0\n",
    "            val_success.append(1)\n",
    "        else:\n",
    "            #val_reward.append(reward)\n",
    "            #if total_rew+1.0 >=0:\n",
    "            #    val_success.append(total_rew+1.0)\n",
    "            #else:\n",
    "            val_success.append(0)\n",
    "        #infor.append([i,step,total_rew])\n",
    "        #print(total_rew)\n",
    "    print(sum(val_success),\" <-- Number of success\")\n",
    "    try:\n",
    "        scenario.close()\n",
    "    except:\n",
    "        pass\n",
    "    scenario = prepare_ngsim_scenario(client)\n",
    "    world = client.get_world()\n",
    "    #self.spectator = self.world.get_spectator()\n",
    "    ego_vehicle = prepare_ego_vehicle(world)\n",
    "    birdview_producer = BirdViewProducer(\n",
    "        client,  # carla.Client\n",
    "        target_size=PixelDimensions(width=186, height=150),\n",
    "        pixels_per_meter=4\n",
    "        #crop_type=BirdViewCropType.FRONT_AREA_ONLY\n",
    "    )\n",
    "\n",
    "    '''\n",
    "    except Exception as e:\n",
    "        print(\"Re init Train:\",e)\n",
    "        self.load_carla()\n",
    "        self.scenario = prepare_ngsim_scenario(self.client)\n",
    "        self.world = self.client.get_world()\n",
    "        #self.spectator = self.world.get_spectator()\n",
    "        self.ego_vehicle = prepare_ego_vehicle(self.world)\n",
    "        self.birdview_producer = BirdViewProducer(\n",
    "            self.client,  # carla.Client\n",
    "            target_size=PixelDimensions(width=150, height=186),\n",
    "            pixels_per_meter=4,\n",
    "            crop_type=BirdViewCropType.FRONT_AREA_ONLY\n",
    "        )\n",
    "    '''\n",
    "    scenario.reset(ego_vehicle)\n",
    "    world.tick() \n",
    "    if val_count == 0:\n",
    "        val_count = 1\n",
    "    val_scores = sum(val_success)/(len(val_success))\n",
    "    if val_scores > max_score:\n",
    "        print(\"Found a max score!\")\n",
    "        max_score = val_scores\n",
    "        p.save(\"BestScoreNModel\"+str(int(max_score))+\".mdl\")\n",
    "    return val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.buffer.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.save(\"SmallHalfModel.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_reward_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_cam = {}\n",
    "for i in range(50):\n",
    "    iter_cam[i] = []\n",
    "\n",
    "def check_img(img,i):\n",
    "    array = np.frombuffer(img.raw_data, dtype=np.dtype(\"uint8\"))\n",
    "    #print(array.shape)\n",
    "    array = np.reshape(array, (img.height, img.width, 4)) # RGBA format\n",
    "    array = array[:, :, :3] #  Take only RGB\n",
    "    #print(array.shape)\n",
    "    #plt.imshow(array)\n",
    "    \n",
    "    img = Image.fromarray(array)\n",
    "    \n",
    "    #print(img)\n",
    "    img = img.resize((320,320), Image.ANTIALIAS)\n",
    "    #print(img)\n",
    "    input_data = np.array(img)\n",
    "    iter_cam[i].append(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario.reset(ego_vehicle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_bp = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "cam_bp.set_attribute(\"image_size_x\",str(320))\n",
    "cam_bp.set_attribute(\"image_size_y\",str(320))\n",
    "cam_bp.set_attribute(\"fov\",str(100))\n",
    "cam_location = carla.Location(2,0,20)\n",
    "cam_rotation = carla.Rotation(-90,0,0)\n",
    "cam_transform = carla.Transform(cam_location,cam_rotation)\n",
    "ego_front_cam = world.spawn_actor(cam_bp,cam_transform,attach_to=ego_vehicle, attachment_type=carla.AttachmentType.Rigid)\n",
    "#self.rgb_front_listener = ego_cam\n",
    "ego_front_cam.listen(lambda image: check_img(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = []\n",
    "for i in range(0,6000,100):\n",
    "    c = i*100\n",
    "    k = total_reward_list[i:i+100]\n",
    "    count = 0\n",
    "    for j in k:\n",
    "        if j>0.0:\n",
    "            count+=1\n",
    "    train_score.append(count)        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(iter_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "198,272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(iter_cam[61])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = []\n",
    "for i in infor:\n",
    "    if i[2]>0:\n",
    "        success.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in success:\n",
    "    imageio.mimsave('Te'+str(i)+'.gif', iter_cam[i], fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in iter_rgb:\n",
    "    imageio.mimsave('NonCommGif/scene'+str(i)+'.gif', iter_rgb[i], fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_rgb.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.ylim(0,1)\n",
    "plt.plot(list(range(len(val_points))),val_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=p.select_action(,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    done = False\n",
    "    #mem = (torch.zeros(1,1,128).to(device),torch.zeros(1,1,128).to(device))\n",
    "    c = 0\n",
    "    while not done:\n",
    "        st = torch.rand(1,1,16)\n",
    "        dt= p.select_action(st)\n",
    "        print(dt)\n",
    "        reward = 0.1\n",
    "        if c >5:\n",
    "            done = True\n",
    "        c+=1\n",
    "        p.buffer.rewards.append(reward)\n",
    "        p.buffer.is_terminals.append(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester[te2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = torch.rand(1,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Categorical(nn.Softmax(dim=-1)(nn.Linear(16,4)(x))).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = z.reshape(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected TensorOptions(dtype=float, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)) (got TensorOptions(dtype=long int, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2cf823d2d0ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: expected TensorOptions(dtype=float, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)) (got TensorOptions(dtype=long int, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)))"
     ]
    }
   ],
   "source": [
    "\n",
    "c = torch.FloatTensor(c).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-35a427d7fff0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Float but found Long"
     ]
    }
   ],
   "source": [
    "nn.Linear(1,5)(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [random.random() for x in range(16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    a = torch.FloatTensor(a)\n",
    "    c = torch.FloatTensor([a,a,a,a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(1,32,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.cat((a, a, a, a, a), 0).reshape(1,5,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scenario._lane_change_instants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd,r,done,_ = scenario.step(ego_vehicle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"val_score2.pkl\",\"wb\")\n",
    "pickle.dump(val_points,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"val_score2.pkl\",'rb')\n",
    "val_points = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in iter_rgb:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.mimsave('video1.gif', iter_cam[205:290], fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(iter_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lane_dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lane_dir = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lane_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"Data.pkl\",\"wb\")\n",
    "pickle.dump(lane_dir,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"Data.pkl\",'rb')\n",
    "lane_dir = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario.reset(ego_vehicle,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = [103,223,519,560]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(743,965):\n",
    "    if i in blacklist:\n",
    "        continue\n",
    "    print(i)\n",
    "    try:\n",
    "        scenario.reset(ego_vehicle,i)\n",
    "        world.tick()\n",
    "        cmd,r,done,_ = scenario.step(ego_vehicle)\n",
    "        world.tick()\n",
    "        lane_dir.append(cmd.value)\n",
    "    except:\n",
    "        scenario = prepare_ngsim_scenario(client)\n",
    "        world = client.get_world()\n",
    "        spectator = world.get_spectator()\n",
    "        ego_vehicle = prepare_ego_vehicle(world)\n",
    "        scenario.reset(ego_vehicle,i)\n",
    "        world.tick()\n",
    "        cmd,r,done,_ = scenario.step(ego_vehicle)\n",
    "        world.tick()\n",
    "        lane_dir.append(cmd.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set(range(1,10))\n",
    "b = set([1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice(list(set(range(965)).difference(set([103,223,519,560]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(range(1,10)) - [1,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lane_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = [x - 4 for x in lane_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = c.reshape(1,5,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    in_data = torch.FloatTensor(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = a.reshape(1,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.layer1(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(10, 20, 2)\n",
    "inp = torch.randn(1, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn(inp)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.layer1(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
