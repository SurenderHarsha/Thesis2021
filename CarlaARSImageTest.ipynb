{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NGSIM_DIR\"] = \"/home/surender/Downloads/NGSIM\"\n",
    "os.environ[\"OPENDD_DIR\"] = \"/home/surender/Downloads/openDD\"\n",
    "os.environ[\"CARLA_PATH\"] = \"/home/surender/Downloads/carlaOld\"\n",
    "import sys\n",
    "#sys.path.append('/home/surender/Downloads/CARLA_0.9.9.4/PythonAPI/carla/dist')\n",
    "import carla\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "from carla_real_traffic_scenarios.carla_maps import CarlaMaps\n",
    "from carla_real_traffic_scenarios.ngsim import NGSimDatasets, DatasetMode\n",
    "from carla_real_traffic_scenarios.ngsim.scenario import NGSimLaneChangeScenario\n",
    "from carla_real_traffic_scenarios.opendd.scenario import OpenDDScenario\n",
    "from carla_real_traffic_scenarios.reward import RewardType\n",
    "from carla_real_traffic_scenarios.scenario import Scenario\n",
    "\n",
    "from carla_birdeye_view import BirdViewProducer, BirdViewCropType, PixelDimensions\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "import cv2\n",
    "\n",
    "%matplotlib tk\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "\n",
    "from tensorflow.python.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "from keras.layers import Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmd_carla():\n",
    "    os.system(\"DISPLAY= /home/surender/Downloads/carlaOld/CarlaUE4.sh -benchmark -fps=10 -quality-level=Low -opengl -Resx=300 -Resy=300 -NoVSync\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def parser_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--dataset\", choices=[\"ngsim\", \"opendd\"], default=\"opendd\")\n",
    "    parser.add_argument(\"--host\", default=\"127.0.0.1\")\n",
    "    parser.add_argument(\"--port\", default=2000, type=int)\n",
    "    parser.add_argument(\"--num-episodes\", default=10, type=int)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "'''\n",
    "\n",
    "def prepare_ngsim_scenario(client: carla.Client) -> Scenario:\n",
    "    data_dir = os.environ.get(\"NGSIM_DIR\")\n",
    "    #data_dir = os.listdir('/home/surender/Downloads/NGSIM')\n",
    "    assert data_dir, \"Path to the directory with NGSIM dataset is required\"\n",
    "    ngsim_map = NGSimDatasets.list()\n",
    "    ngsim_dataset = ngsim_map[1]\n",
    "    client.load_world(ngsim_dataset.carla_map.level_path)\n",
    "    return NGSimLaneChangeScenario(\n",
    "        ngsim_dataset,\n",
    "        dataset_mode=DatasetMode.TRAIN,\n",
    "        data_dir=data_dir,\n",
    "        reward_type=RewardType.DENSE,\n",
    "        client=client,\n",
    "    )\n",
    "\n",
    "'''\n",
    "def prepare_opendd_scenario(client: carla.Client) -> Scenario:\n",
    "    data_dir = os.environ.get(\"OPENDD_DIR\")\n",
    "    assert data_dir, \"Path to the directory with openDD dataset is required\"\n",
    "    maps = [\"rdb1\", \"rdb2\", \"rdb3\", \"rdb4\", \"rdb5\", \"rdb6\", \"rdb7\"]\n",
    "    map_name = random.choice(maps)\n",
    "    carla_map = getattr(CarlaMaps, map_name.upper())\n",
    "    client.load_world(carla_map.level_path)\n",
    "    return OpenDDScenario(\n",
    "        client,\n",
    "        dataset_dir=data_dir,\n",
    "        dataset_mode=DatasetMode.TRAIN,\n",
    "        reward_type=RewardType.DENSE,\n",
    "        place_name=map_name,\n",
    "    )\n",
    "\n",
    "'''\n",
    "def prepare_ego_vehicle(world: carla.World) -> carla.Actor:\n",
    "    car_blueprint = world.get_blueprint_library().find(\"vehicle.audi.a2\")\n",
    "\n",
    "    # This will allow external scripts like manual_control.py or no_rendering_mode.py\n",
    "    # from the official CARLA examples to take control over the ego agent\n",
    "    car_blueprint.set_attribute(\"role_name\", \"hero\")\n",
    "\n",
    "    # spawn points doesnt matter - scenario sets up position in reset\n",
    "    ego_vehicle = world.spawn_actor(\n",
    "        car_blueprint, carla.Transform(carla.Location(0, 0, 500), carla.Rotation())\n",
    "    )\n",
    "\n",
    "    assert ego_vehicle is not None, \"Ego vehicle could not be spawned\"\n",
    "\n",
    "    # Setup any car sensors you like, collect observations and then use them as input to your model\n",
    "    return ego_vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = InceptionResNetV2(weights='imagenet', include_top=True)\n",
    "model = Model(inputs=base_model.input,outputs=base_model.get_layer('avg_pool').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = threading.Thread(target = cmd_carla)\n",
    "p.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"localhost\"\n",
    "port = 2000\n",
    "client = carla.Client(host,port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = prepare_ngsim_scenario(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = client.get_world()\n",
    "spectator = world.get_spectator()\n",
    "ego_vehicle = prepare_ego_vehicle(world)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features  = 0\n",
    "final_features1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "def get_features(img):\n",
    "    \n",
    "    global final_features,model\n",
    "    array = np.frombuffer(img.raw_data, dtype=np.dtype(\"uint8\")) \n",
    "    array = np.reshape(array, (img.height, img.width, 4)) # RGBA format\n",
    "    array = array[:, :, :3] #  Take only RGB\n",
    "    \n",
    "    \n",
    "    img = Image.fromarray(array)\n",
    "    img = img.resize((299, 299), Image.ANTIALIAS)\n",
    "    input_data = np.array(img)\n",
    "    #print(\"GOOD\")\n",
    "    x = np.expand_dims(input_data, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    #print(\"FINE\")\n",
    "    features = model.predict(x) \n",
    "    #print(\"WHY\")\n",
    "    features = features[0]\n",
    "    #print(\"WORKS?\")\n",
    "    final_features = features\n",
    "    \n",
    "def get_features1(img):\n",
    "    global final_features1,model\n",
    "    array = np.frombuffer(img.raw_data, dtype=np.dtype(\"uint8\")) \n",
    "    array = np.reshape(array, (img.height, img.width, 4)) # RGBA format\n",
    "    array = array[:, :, :3] #  Take only RGB\n",
    "    img = Image.fromarray(array)\n",
    "    img = img.resize((299, 299), Image.ANTIALIAS)\n",
    "    input_data = np.array(img)\n",
    "    #print(\"NO\")\n",
    "    x = np.expand_dims(input_data, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    #print(\"MAYBE\")\n",
    "    features = model.predict(x) \n",
    "    #print(\"WHI NOS\")\n",
    "    features = features[0]\n",
    "    \n",
    "    final_features1 = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "birdview_producer = BirdViewProducer(\n",
    "    client,  # carla.Client\n",
    "    target_size=PixelDimensions(width=150, height=336),\n",
    "    pixels_per_meter=4,\n",
    "    crop_type=BirdViewCropType.FRONT_AND_REAR_AREA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_carla():\n",
    "    global host,port,client,scenario, world,spectator,ego_vehicle,birdview_producers,p\n",
    "    p = threading.Thread(target = cmd_carla)\n",
    "    p.start()\n",
    "    print(\"Restarting carla.....\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "    \n",
    "    print(\"Connecting to carla...\")\n",
    "    host = \"localhost\"\n",
    "    port = 2000\n",
    "    client = carla.Client(host,port)\n",
    "    scenario = prepare_ngsim_scenario(client)\n",
    "    world = client.get_world()\n",
    "    spectator = world.get_spectator()\n",
    "    ego_vehicle = prepare_ego_vehicle(world)\n",
    "    birdview_producer = BirdViewProducer(\n",
    "    client,  # carla.Client\n",
    "    target_size=PixelDimensions(width=150, height=336),\n",
    "    pixels_per_meter=4,\n",
    "    crop_type=BirdViewCropType.FRONT_AND_REAR_AREA\n",
    "    )\n",
    "    \n",
    "    \n",
    "    cam_bp = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "    cam_bp.set_attribute(\"image_size_x\",str(320))\n",
    "    cam_bp.set_attribute(\"image_size_y\",str(320))\n",
    "    cam_bp.set_attribute(\"fov\",str(100))\n",
    "    cam_location = carla.Location(2,0,1)\n",
    "    cam_rotation = carla.Rotation(0,0,0)\n",
    "    cam_transform = carla.Transform(cam_location,cam_rotation)\n",
    "    ego_cam = world.spawn_actor(cam_bp,cam_transform,attach_to=ego_vehicle, attachment_type=carla.AttachmentType.Rigid)\n",
    "    ego_cam.listen(lambda image: get_features(image))\n",
    "    \n",
    "    cam_bp1 = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "    cam_bp1.set_attribute(\"image_size_x\",str(320))\n",
    "    cam_bp1.set_attribute(\"image_size_y\",str(320))\n",
    "    cam_bp1.set_attribute(\"fov\",str(100))\n",
    "    cam_location1 = carla.Location(-2,0,1)\n",
    "    cam_rotation1 = carla.Rotation(0,180,0)\n",
    "    cam_transform1 = carla.Transform(cam_location1,cam_rotation1)\n",
    "    ego_cam1 = world.spawn_actor(cam_bp1,cam_transform1,attach_to=ego_vehicle, attachment_type=carla.AttachmentType.Rigid)\n",
    "    ego_cam1.listen(lambda image1: get_features1(image1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hp():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nb_steps = 1000\n",
    "        self.episode_length = 1000\n",
    "        self.learning_rate = 0.02\n",
    "        self.nb_directions = 16\n",
    "        self.nb_best_directions = 16\n",
    "        assert self.nb_best_directions <= self.nb_directions\n",
    "        self.noise = 0.03\n",
    "        self.seed = 10\n",
    "        #self.env_name = 'HalfCheetahBulletEnv-v0'\n",
    "\n",
    "# Normalizing the states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = Hp()\n",
    "np.random.seed(hp.seed)\n",
    "state_limit = 15\n",
    "\n",
    "\n",
    "input_state = [0]*state_limit*4\n",
    "nb_inputs = 3072\n",
    "nb_outputs = 2\n",
    "save_every = 25\n",
    "\n",
    "render = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_vector = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_bp = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "cam_bp.set_attribute(\"image_size_x\",str(320))\n",
    "cam_bp.set_attribute(\"image_size_y\",str(320))\n",
    "cam_bp.set_attribute(\"fov\",str(100))\n",
    "cam_location = carla.Location(2,0,1)\n",
    "cam_rotation = carla.Rotation(0,0,0)\n",
    "cam_transform = carla.Transform(cam_location,cam_rotation)\n",
    "ego_cam = world.spawn_actor(cam_bp,cam_transform,attach_to=ego_vehicle, attachment_type=carla.AttachmentType.Rigid)\n",
    "ego_cam.listen(lambda image: get_features(image))\n",
    "\n",
    "cam_bp1 = world.get_blueprint_library().find('sensor.camera.rgb')\n",
    "cam_bp1.set_attribute(\"image_size_x\",str(320))\n",
    "cam_bp1.set_attribute(\"image_size_y\",str(320))\n",
    "cam_bp1.set_attribute(\"fov\",str(100))\n",
    "cam_location1 = carla.Location(-2,0,1)\n",
    "cam_rotation1 = carla.Rotation(0,180,0)\n",
    "cam_transform1 = carla.Transform(cam_location1,cam_rotation1)\n",
    "ego_cam1 = world.spawn_actor(cam_bp1,cam_transform1,attach_to=ego_vehicle, attachment_type=carla.AttachmentType.Rigid)\n",
    "ego_cam1.listen(lambda image1: get_features1(image1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.tick()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Normalizer():\n",
    "    \n",
    "    def __init__(self, nb_inputs):\n",
    "        global hp\n",
    "        self.n = np.zeros(nb_inputs)\n",
    "        self.mean = np.zeros(nb_inputs)\n",
    "        self.mean_diff = np.zeros(nb_inputs)\n",
    "        self.var = np.zeros(nb_inputs)\n",
    "    \n",
    "    def observe(self, x):\n",
    "        global hp\n",
    "        self.n += 1.\n",
    "        last_mean = self.mean.copy()\n",
    "        \n",
    "        self.mean += (x - self.mean) / self.n\n",
    "        \n",
    "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
    "        self.var = (self.mean_diff / self.n).clip(min = 1e-2)\n",
    "    \n",
    "    def normalize(self, inputs):\n",
    "        global hp\n",
    "        obs_mean = self.mean\n",
    "        obs_std = np.sqrt(self.var)\n",
    "        if np.sum(obs_std) == 0:\n",
    "            print(\"TILTED\")\n",
    "        return (inputs - obs_mean) / obs_std\n",
    "\n",
    "# Building the AI\n",
    "\n",
    "class Policy():\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        \n",
    "        self.theta = np.zeros((output_size, input_size))\n",
    "    \n",
    "    def evaluate(self, input, delta = None, direction = None):\n",
    "        global hp\n",
    "        if direction is None:\n",
    "            return self.theta.dot(input)\n",
    "        elif direction == \"positive\":\n",
    "            return (self.theta + hp.noise*delta).dot(input)\n",
    "        else:\n",
    "            return (self.theta - hp.noise*delta).dot(input)\n",
    "    \n",
    "    def sample_deltas(self):\n",
    "        global hp\n",
    "        return [np.random.randn(*self.theta.shape) for _ in range(hp.nb_directions)]\n",
    "    \n",
    "    def update(self, rollouts, sigma_r):\n",
    "        global hp\n",
    "        step = np.zeros(self.theta.shape)\n",
    "        for r_pos, r_neg, d in rollouts:\n",
    "            step += (r_pos - r_neg) * d\n",
    "        \n",
    "        if (sigma_r == 0 or np.sum(step) ==0):\n",
    "            self.theta += hp.learning_rate / 2\n",
    "        else:\n",
    "            self.theta += hp.learning_rate / (hp.nb_best_directions * sigma_r) * step\n",
    "        \n",
    "    def save_policy(self,step_num):\n",
    "        np.savetxt(\"Policy_\" + str(step_num) +\".gz\", self.theta)\n",
    "    \n",
    "    def load_policy(self,step_num):\n",
    "        self.theta = np.loadtxt(\"Policy_\" + str(step_num) +\".gz\")\n",
    "\n",
    "# Exploring the policy on one specific direction and over one episode\n",
    "\n",
    "def explore(env, normalizer, policy, direction = None, delta = None):\n",
    "    global ego_vehicle,input_state,render,hp, final_features, final_features1\n",
    "    env.reset(ego_vehicle)\n",
    "    world.tick()\n",
    "    state = list(np.append(final_features, final_features1))\n",
    "    done = False\n",
    "    num_plays = 0.\n",
    "    sum_rewards = 0\n",
    "    while not done:\n",
    "        normalizer.observe(state)\n",
    "        state = normalizer.normalize(state)\n",
    "        action = policy.evaluate(state, delta, direction)\n",
    "        action[0] = np.clip(action[0],0.0,1.0) # Throttle\n",
    "        action[1] = np.clip(action[1],-1.0,1.0) # Steering\n",
    "        #action[2] = np.clip(action[2],0.0,1.0) #Brake\n",
    "        \n",
    "        if render:\n",
    "            birdview = birdview_producer.produce(\n",
    "            agent_vehicle=ego_vehicle  # carla.Actor (spawned vehicle)\n",
    "            )\n",
    "            rgb = BirdViewProducer.as_rgb(birdview)\n",
    "            cv2.imshow('Frame',rgb)\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "\n",
    "                  break\n",
    "        \n",
    "        ego_vehicle.apply_control(carla.VehicleControl(throttle=action[0], steer=action[1]))\n",
    "        cmd, reward, done, _ = env.step(ego_vehicle)\n",
    "        #reward = np.clip(reward,0,1)\n",
    "        #reward = max(reward, -1)\n",
    "        #reward = max(min(reward, 1), -1)\n",
    "        world.tick()\n",
    "        sum_rewards += reward\n",
    "        num_plays += 1\n",
    "    cv2.destroyAllWindows()\n",
    "    return sum_rewards\n",
    "\n",
    "\n",
    "def train(env, policy, normalizer, hp, resume=None, resume_step = 0):\n",
    "    global render,save_every,reward_vector,scenario\n",
    "    max_reward =  -1\n",
    "    if resume == True:\n",
    "        policy.load_policy(resume_step)\n",
    "        \n",
    "    for step in range(resume_step, hp.nb_steps):\n",
    "        try:\n",
    "            if step > 150:\n",
    "                render = True\n",
    "            # Initializing the perturbations deltas and the positive/negative rewards\n",
    "            deltas = policy.sample_deltas()\n",
    "            positive_rewards = [0] * hp.nb_directions\n",
    "            negative_rewards = [0] * hp.nb_directions\n",
    "\n",
    "            # Getting the positive rewards in the positive directions\n",
    "            for k in range(hp.nb_directions):\n",
    "                positive_rewards[k] = explore(env, normalizer, policy, direction = \"positive\", delta = deltas[k])\n",
    "\n",
    "            # Getting the negative rewards in the negative/opposite directions\n",
    "            for k in range(hp.nb_directions):\n",
    "                negative_rewards[k] = explore(env, normalizer, policy, direction = \"negative\", delta = deltas[k])\n",
    "\n",
    "            # Gathering all the positive/negative rewards to compute the standard deviation of these rewards\n",
    "            all_rewards = np.array(positive_rewards + negative_rewards)\n",
    "            sigma_r = all_rewards.std()\n",
    "\n",
    "            # Sorting the rollouts by the max(r_pos, r_neg) and selecting the best directions\n",
    "            scores = {k:max(r_pos, r_neg) for k,(r_pos,r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n",
    "            order = sorted(scores.keys(), key = lambda x:scores[x], reverse = True)[:hp.nb_best_directions]\n",
    "            rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order]\n",
    "\n",
    "            # Updating our policy\n",
    "            policy.update(rollouts, sigma_r)\n",
    "\n",
    "            if step%save_every == 0:\n",
    "                policy.save_policy(step)\n",
    "\n",
    "\n",
    "            # Printing the final reward of the policy after the update\n",
    "            reward_evaluation = explore(env, normalizer, policy)\n",
    "\n",
    "            if reward_evaluation > max_reward:\n",
    "                policy.save_policy(step)\n",
    "                print(\"Max reward:\",reward_evaluation)\n",
    "                max_reward = reward_evaluation\n",
    "            print('Step:', step, 'Reward:', reward_evaluation)\n",
    "            reward_vector.append(reward_evaluation)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            reload_carla()\n",
    "            time.sleep(5)\n",
    "            reward_vector.append(reward_vector[-1])\n",
    "            env = scenario\n",
    "            print(\"Carla Reloaded and connected, continue training!\")\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_radar(dat):\n",
    "    global input_state,state_limit\n",
    "    points = np.frombuffer(dat.raw_data, dtype=np.dtype('f4'))\n",
    "    points = np.reshape(points, (len(dat), 4))\n",
    "    radar_Data = points[np.argsort(points[:, -1])]\n",
    "        \n",
    "    state = radar_Data[:state_limit].flatten()\n",
    "    state.resize(state_limit*4,refcheck = False)\n",
    "    input_state =  state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ldr_bp = world.get_blueprint_library().find('sensor.other.radar')\n",
    "ldr_loc = carla.Location(0,0,0)\n",
    "ldr_rot = carla.Rotation(0,0,0)\n",
    "ldr_bp.set_attribute(\"range\",'10.0')\n",
    "\n",
    "\n",
    "ldr_transform = carla.Transform(ldr_loc,ldr_rot)\n",
    "ego_ldr = world.spawn_actor(ldr_bp,ldr_transform, attach_to=ego_vehicle, attachment_type=carla.AttachmentType.Rigid)\n",
    "ego_ldr.listen(lambda dat: load_radar(dat))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy(nb_inputs, nb_outputs)\n",
    "normalizer = Normalizer(nb_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(scenario, policy, normalizer, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_array.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(10,4).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.resize(50,refcheck=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
